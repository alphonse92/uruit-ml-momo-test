{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "k899AHfCA9AK"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook shows how to create a neural networ to detect momo in images using Inception V3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ma3EfL5ZA9AL"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bXr9oRv3A9AM",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import datetime; \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import csv\n",
        "from time import time\n",
        "\n",
        "from shutil import copy2, rmtree, copytree\n",
        "from tqdm import tqdm\n",
        "from sys import stdout\n",
        "from os import listdir, makedirs, remove\n",
        "from os.path import isfile, join, isdir, exists, dirname, abspath\n",
        "from inspect import getsourcefile\n",
        "from tensorflow import keras\n",
        "from numpy.random import seed\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from itertools import product\n",
        "\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OoR-dXgQjVRm"
      },
      "source": [
        "# Downloading dataset from Github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EzUBb-HPjVRn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "3cb93dbb-6340-45c1-adec-71680d989f66"
      },
      "source": [
        "![ ! -d \"dataset\" ] && echo \"Cloning dataset project from github\" && git clone https://github.com/alphonse92/momo-dataset.git ./dataset \n",
        "![ ! -d \"visualization\" ] && echo \"Cloning visualization repo from github\" && git clone https://github.com/alphonse92/uruit-ml-momo-test-visualization visualization \n",
        "\n",
        "!cd dataset       && git reset --hard && git pull && git checkout V2\n",
        "!cd visualization && git reset --hard && git pull\n",
        "!rm -rf result/ && cp -r visualization/result ./result"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HEAD is now at 8f85f22 clean raw data\n",
            "Already up to date.\n",
            "Already on 'V2'\n",
            "Your branch is up to date with 'origin/V2'.\n",
            "HEAD is now at 2032732 get more data\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "__Gb10ewA9AQ",
        "colab": {}
      },
      "source": [
        "# Load Inception V3.\n",
        "\n",
        "InceptionV3             = keras.applications.inception_v3.InceptionV3\n",
        "     \n",
        "preprocess_input        = keras.applications.inception_v3.preprocess_input\n",
        "image                   = keras.preprocessing.image\n",
        "     \n",
        "ImageDataGenerator      = keras.preprocessing.image.ImageDataGenerator\n",
        "Callback                = keras.callbacks\n",
        "     \n",
        "Model                   = keras.models.Model\n",
        "Sequential              = keras.models.Sequential\n",
        "layers                  = keras.layers\n",
        "     \n",
        "Input                   = layers.Input\n",
        "Dense                   = layers.Dense\n",
        "Conv2D                  = layers.Conv2D\n",
        "ZeroPadding2D           = layers.ZeroPadding2D\n",
        "BatchNormalization      = layers.BatchNormalization\n",
        "AveragePooling2D        = layers.AveragePooling2D\n",
        "MaxPooling2D            = layers.MaxPooling2D\n",
        "GlobalAveragePooling2D  = layers.GlobalAveragePooling2D\n",
        "Dropout                 = layers.Dropout\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "chxHt9JvA9AV"
      },
      "source": [
        "# Notebook Configuration \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nKRDESWDA9AV",
        "colab": {}
      },
      "source": [
        "# Notebook Configuration\n",
        "USE_INCEPTION_V3_MODEL = True\n",
        "RESET_TRAINING_EXAMPLES = True\n",
        "SAVE_WEIGHTS = True\n",
        "SEED_APP = 9\n",
        "\n",
        "# DATASET PROPORTIONS\n",
        "TRAINING_PERCENTAGE = 0.7\n",
        "TESTING_PERCENTAGE  = 0.15\n",
        "EVAL_PERCENTAGE     = 0.15\n",
        "\n",
        "# HYPERPARAMETERS\n",
        "CRITERIA_THRESHOLD = 0.7\n",
        "EPOCHS = 10\n",
        "EPOCHS_BOTTLENECK = 100\n",
        "BATCH_SIZE = 15\n",
        "IMG_W = IMG_H = 299\n",
        "\n",
        "# MODEL CHECKPOINTS CONFIGURATION\n",
        "SAVE_CHECKPOINTS = 2\n",
        "SAVE_WEIGHTS_FREQUENCY = 'epoch'\n",
        "SAVE_WEIGTHS_PERIOD = EPOCHS // SAVE_CHECKPOINTS\n",
        "\n",
        "# COLAB SCOPE\n",
        "MOUNT_G_DRIVE = False\n",
        "GOOGLE_COLLAB = False                                                            # Default value. You should not modify this\n",
        "GOOGLE_RESET_CONTENT_TREE = True                                                 # Reset the dataset content tree. It means remove and re copy the data from drive\n",
        "MOUNT = \"./\"                                                                     # Base path of this project. You may not change this value   \n",
        "G_MOUNT = \"/content/drive\"                                                       # Set where the drive folder will be mounted\n",
        "G_PROJECT_PATH = G_MOUNT+ \"/My Drive/Colab Notebooks/uruit-ml-momo-test/\"        # Set the pathe where momo project is \n",
        "\n",
        "# If colab instance, then build the content tree\n",
        "try:\n",
        "  import google.colab\n",
        "  from google.colab import drive\n",
        "  if MOUNT_G_DRIVE: \n",
        "    drive.mount('/content/drive', force_remount=GOOGLE_RESET_CONTENT_TREE)\n",
        "  GOOGLE_COLLAB = True\n",
        "except:\n",
        "  GOOGLE_COLLAB = False\n",
        "  print(tf.test.gpu_device_name())\n",
        "  "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3Pp1IbKlA9AX"
      },
      "source": [
        "# Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cLOM_Um3A9AY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "44983db4-2c52-46b7-a460-0fd224feb418"
      },
      "source": [
        "tf.random.set_seed(SEED_APP)\n",
        "\n",
        "MOMO_CLASSNAME    = \"momo\"\n",
        "NO_MOMO_CLASSNAME = \"no_momo\"\n",
        "\n",
        "DATASET_PATH = join(MOUNT, \"dataset/\")\n",
        "DATESET_BASIC_PATH    = join(DATASET_PATH,'basic/')\n",
        "DATESET_TRAINING_PATH = join(DATASET_PATH,'train/')\n",
        "DATESET_TESTING_PATH  = join(DATASET_PATH,'test/')\n",
        "DATESET_EVAL_PATH     = join(DATASET_PATH,'eval/')\n",
        "\n",
        "RESULT_FOLDER_PATH = join(MOUNT,\"result/\")\n",
        "VISUALIZATION_FOLDER = RESULT_FOLDER_PATH + \"basic-predictions/\"\n",
        "VISUALIZATION_LATEST_RESULT_FOLDER = \"latest/\"\n",
        "\n",
        "INCEPTION_V3_MODEL_NAME = \"inception_v3\"\n",
        "BOTTLENECK_MODEL_NAME   = \"bottleneck\"\n",
        "\n",
        "RESULT_FOLDER_WEIGHTS_INCEPTION_V3_PATH = RESULT_FOLDER_PATH + INCEPTION_V3_MODEL_NAME + \"/\"\n",
        "RESULT_FOLDER_WEIGHTS_BOTTLENECK_PATH = RESULT_FOLDER_PATH +  INCEPTION_V3_MODEL_NAME+ \"_\" + BOTTLENECK_MODEL_NAME + \"/\"\n",
        "\n",
        "DEFAULT_WEIGHTS_FILE_NAME = 'weights.h5'\n",
        "DEFAULT_WEIGHTS_FILE_PATH = RESULT_FOLDER_WEIGHTS_INCEPTION_V3_PATH + DEFAULT_WEIGHTS_FILE_NAME\n",
        "RESULT_FOLDER_WEIGHTS_BOTTLENECK_FILE_NAME = RESULT_FOLDER_WEIGHTS_BOTTLENECK_PATH + DEFAULT_WEIGHTS_FILE_NAME\n",
        "\n",
        "  \n",
        "performanceDF = pd.DataFrame()\n",
        "\n",
        "print(DATESET_BASIC_PATH)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./dataset/basic/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OPyDA1SlA9Ad"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pU3NLf7IA9Ae",
        "colab": {}
      },
      "source": [
        "class ComposedBottleneckModel:\n",
        " \n",
        "  def __init__(self,BaseModel,BottleneckModel):\n",
        "      self.BaseModel = BaseModel            \n",
        "      self.BottleneckModel = BottleneckModel\n",
        "\n",
        "  def predict(self,imageArray):\n",
        "    embdings   = self.BaseModel.predict(imageArray)\n",
        "    prediction = self.BottleneckModel.predict(embdings)\n",
        "    return prediction\n",
        "\n",
        "\n",
        "def getDistribuitions(array,TRAINING_PERCENTAGE,TESTING_PERCENTAGE,EVAL_PERCENTAGE):\n",
        "  m         = len(array)\n",
        "  trainIdx  = math.ceil( m * TRAINING_PERCENTAGE )\n",
        "  testIdx   = math.ceil( m * TESTING_PERCENTAGE  ) \n",
        "  evalIdx   = math.ceil( m * EVAL_PERCENTAGE     )     \n",
        "\n",
        "  np.random.seed(SEED_APP)\n",
        "  np.random.shuffle(array)\n",
        "\n",
        "  currentIndex     = 0\n",
        "  trainingImages   = array[ currentIndex : currentIndex + trainIdx ]\n",
        "\n",
        "  currentIndex     = currentIndex + trainIdx \n",
        "  testImages       = array[ currentIndex : currentIndex + testIdx  ]\n",
        "\n",
        "  currentIndex     = currentIndex + testIdx\n",
        "  evalImages       = array[ currentIndex :          ]\n",
        "\n",
        "  return {\n",
        "      'train':trainingImages,\n",
        "      'test' :testImages,\n",
        "      'eval' :evalImages\n",
        "  }\n",
        "\n",
        "def getFolders(path):\n",
        "    return [d for d in listdir(path) if isdir(join(path, d))]\n",
        "\n",
        "def getFolderFiles(path: str):\n",
        "    return [f for f in listdir(path) if isfile(join(path, f))]  \n",
        "\n",
        "\n",
        "def predict(path, model) -> np.array:\n",
        "    img = image.load_img(path, target_size=(299, 299))\n",
        "    # Size  (299, 299, 3)\n",
        "    imgArray = image.img_to_array(img) \n",
        "    \n",
        "    # Size  (1, 299, 299, 3)\n",
        "    expandedImgArray = np.expand_dims(imgArray, axis=0) \n",
        "    \n",
        "    # Preproces to inceptionV3, normalize each pixel RGB value to an scale of zero to one\n",
        "    processedImgArray = preprocess_input(expandedImgArray) \n",
        "    \n",
        "    return model.predict(processedImgArray)\n",
        "\n",
        "def getTimestamp():\n",
        "    return datetime.datetime.now().timestamp()\n",
        "    \n",
        "def getRandomExample(xClass:str):\n",
        "    np.random.seed(SEED_APP)\n",
        "    exampleFileList = getFolderFiles(DATESET_BASIC_PATH + xClass)\n",
        "    \n",
        "    rndIndex = np.random.randint(0,len(exampleFileList))\n",
        "    filename = exampleFileList[rndIndex]\n",
        "    return join(DATESET_BASIC_PATH,xClass,filename)\n",
        "\n",
        "def getDatasetClasses():\n",
        "    return getFolders(DATESET_BASIC_PATH)\n",
        "\n",
        "def getOutputClasses():\n",
        "    return [NO_MOMO_CLASSNAME,MOMO_CLASSNAME]\n",
        "    \n",
        "def createFolderIfNotExist(folderPath):\n",
        "    if not exists(folderPath):\n",
        "        makedirs(folderPath)\n",
        "\n",
        "def deleteIfExist(filepath):\n",
        "    if exists(filepath):\n",
        "        rmtree(filepath)\n",
        "        \n",
        "def resetFolderIfExist(path : str):\n",
        "    deleteIfExist(path)\n",
        "    createFolderIfNotExist(path)\n",
        "\n",
        "def saveInFileIfNotExist(filepath: str, content: str):\n",
        "  \n",
        "    # Create (or not) the result folder\n",
        "    createFolderIfNotExist(dirname(filepath))\n",
        "    \n",
        "    with open(filepath, mode=\"a\") as f:\n",
        "        f.write(content + '\\n')\n",
        "        \n",
        "\n",
        "def plotModelTrainingProgress(progress):\n",
        "  # Acc stats\n",
        "  plt.plot(progress.history['accuracy'])\n",
        "  plt.plot(progress.history['val_accuracy'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.xlim(0, 1)\n",
        "  plt.ylim(0, 1)\n",
        "  plt.show()\n",
        "\n",
        "  # Loss stats\n",
        "  plt.plot(progress.history['loss'])\n",
        "  plt.plot(progress.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.xlim(0, 1)\n",
        "  plt.ylim(0, 1)\n",
        "  plt.show()\n",
        "  \n",
        "def createConfusionMatrix(y_true, y_pred, classes,\n",
        "                          normalize=False,\n",
        "                          title=None,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]                \n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "    \n",
        "def getDataForConfusionMatrix(model):\n",
        "    threshold = CRITERIA_THRESHOLD\n",
        "    eval_Xs = []\n",
        "    eval_Ys = []\n",
        "    eval_preds = []\n",
        "    files = []\n",
        "\n",
        "    #folderPath = DATESET_TRAINING_PATH\n",
        "    folderPath = DATESET_EVAL_PATH\n",
        "\n",
        "    folders = getFolders(folderPath)\n",
        "\n",
        "    for folder in folders:\n",
        "\n",
        "        images                = getFolderFiles(folderPath+ folder)\n",
        "        totalOfImagesInFolder = len(images)\n",
        "\n",
        "        print(\"Processing\" ,  folder , \" total of images: \",totalOfImagesInFolder)\n",
        "\n",
        "        for img in tqdm(images, file=stdout):\n",
        "            src = folderPath + folder + \"/\" + img        \n",
        "            files.append(src)\n",
        "            result = predict(src,model)[0][0]\n",
        "\n",
        "            # generamos una lista con las probabilidades devueltas por el modelo para cada imagen.\n",
        "            eval_preds.append(result)\n",
        "\n",
        "            # generamos nuestra lista de y_true, es decir la etiqueta de quebería tener la foto.\n",
        "            eval_Xs.append(1 if folder == MOMO_CLASSNAME else 0)\n",
        "\n",
        "            # generamos nuestra lista de y_pred, es decir la etiqueta que nos dio la predicción.\n",
        "            eval_Ys.append(1 if result > threshold else 0)\n",
        "\n",
        "    files      = np.array(files)\n",
        "    eval_preds = np.array(eval_preds)\n",
        "    eval_Xs    = np.array(eval_Xs)\n",
        "    eval_Ys    = np.array(eval_Ys)\n",
        "    \n",
        "    momoModel.load_weights(DEFAULT_WEIGHTS_FILE_PATH)\n",
        "    return (files,eval_preds,eval_Xs,eval_Ys)\n",
        "\n",
        "\n",
        "def printConfussionMatrix(model):\n",
        "  files,eval_preds,eval_Xs,eval_Ys = getDataForConfusionMatrix(model)\n",
        "  createConfusionMatrix(\n",
        "    eval_Xs,\n",
        "    eval_Ys,\n",
        "    title=\"Using bottleneck\",\n",
        "    classes=getOutputClasses(),\n",
        "    normalize=True\n",
        ")\n",
        "        \n",
        "def createInceptionModel():\n",
        "  baseModel = InceptionV3(weights='imagenet', include_top=False, pooling=  'avg')\n",
        "  baseModel.trainable = False\n",
        "  \n",
        "  # Take the output of the model\n",
        "  x = baseModel.output\n",
        "\n",
        "  # Add a full-conected layer of 1024 neurons with relu activation to our model output\n",
        "  x = Dense(1024, activation='relu')(x)\n",
        "  x = layers.Dropout(0.1)(x)\n",
        "  #x = Dense(512, activation='relu')(x)\n",
        "\n",
        "  # Add a output layer with only one neurone\n",
        "  momoOutput = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "  # Create the momo Model from our outputs\n",
        "  model = Model(inputs=baseModel.input, outputs=momoOutput)\n",
        "\n",
        "  for layer in baseModel.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "  # Compile our model using adam and an optimizer for binari clasification\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "  return model\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8320VMXlA9Aj"
      },
      "source": [
        "# Preparing InceptionV3 Model to fit it to our problem: identify momo in. images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "raw",
        "id": "bHQU4JM1A9Ak"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VtMS9lQEA9Ak",
        "colab": {}
      },
      "source": [
        "momoModel = createInceptionModel()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "unM9tz7kA9An",
        "colab": {}
      },
      "source": [
        "#print(momoModel.summary())"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GFMSiiU1A9Ap"
      },
      "source": [
        "# Pre training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3ZaWxM93A9Ap"
      },
      "source": [
        "## Prepare our test/training folders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "73006ufKA9Aq",
        "colab": {}
      },
      "source": [
        "# Read each class of the dataset\n",
        "for ds_class in getOutputClasses():\n",
        "    # Create the folders in the train/test folders\n",
        "    resetFolderIfExist( DATESET_TRAINING_PATH  +  ds_class  )\n",
        "    resetFolderIfExist( DATESET_TESTING_PATH   +  ds_class  )\n",
        "    resetFolderIfExist( DATESET_EVAL_PATH      +  ds_class  )\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ud7r12EdA9As"
      },
      "source": [
        "## Fill the train/test/eval folders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UYF0M09zA9At",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5ca6f051-6fb4-4684-a9a8-7bc8b3908e10"
      },
      "source": [
        "def CreateTrainTestAndEvalFolders():\n",
        "    np.random.seed(SEED_APP)\n",
        "    DATASET_CLASSES = getDatasetClasses()\n",
        "\n",
        "    ds_folders = getFolders(DATESET_BASIC_PATH)\n",
        "\n",
        "    for ds_folder in ds_folders:\n",
        "\n",
        "        path      = DATESET_BASIC_PATH + ds_folder\n",
        "        files     = np.array(getFolderFiles(path))\n",
        "\n",
        "\n",
        "        distributions = getDistribuitions(files,\n",
        "                          TRAINING_PERCENTAGE,\n",
        "                          TESTING_PERCENTAGE,\n",
        "                          EVAL_PERCENTAGE\n",
        "        )\n",
        "\n",
        "        trainingImages = distributions[\"train\"]\n",
        "        testImages     = distributions[\"test\"]\n",
        "        evalImages     = distributions[\"eval\"]\n",
        "        \n",
        "        isPositiveClass      = ds_folder == MOMO_CLASSNAME\n",
        "        folderTo             = MOMO_CLASSNAME if isPositiveClass else NO_MOMO_CLASSNAME\n",
        "\n",
        "        trainingClassPath    = DATESET_TRAINING_PATH + folderTo\n",
        "        testClassPath        = DATESET_TESTING_PATH  + folderTo\n",
        "        evalClassPath        = DATESET_EVAL_PATH     + folderTo\n",
        "\n",
        "        resetFolderIfExist(trainingClassPath)\n",
        "        resetFolderIfExist(testClassPath)\n",
        "        resetFolderIfExist(evalClassPath)\n",
        "\n",
        "\n",
        "        print(\n",
        "            \"From folder '\" + ds_folder  + \"'\"\n",
        "            + \" take \" \n",
        "            + str(len(trainingImages)) + \" Training examples, \"\n",
        "            + str(len(testImages))     + \" Testing examples, and \"\n",
        "            + str(len(evalImages))     + \" Eval examples.\"\n",
        "            + \"\\n\"        \n",
        "        )\n",
        "\n",
        "        print(\"Copying traning files from \" + path + \" to \" + trainingClassPath)\n",
        "        for imageName in tqdm(trainingImages, file=stdout):\n",
        "            copy2(path+ \"/\"+  imageName ,trainingClassPath + \"/\"+  imageName)\n",
        "\n",
        "        print(\"Copying testing files from \" + path + \" to \" + testClassPath)\n",
        "        for imageName in tqdm(testImages, file=stdout):\n",
        "            copy2(path+ \"/\"+  imageName ,testClassPath + \"/\"+  imageName)\n",
        "\n",
        "        print(\"Copying eval files from \"    + path + \" to \" + evalClassPath)\n",
        "        for imageName in tqdm(evalImages, file=stdout):\n",
        "            copy2(path+ \"/\"+  imageName ,evalClassPath + \"/\"+  imageName)\n",
        "        print()\n",
        "        print()\n",
        "            \n",
        "CreateTrainTestAndEvalFolders()\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From folder 'meme' take 91 Training examples, 20 Testing examples, and 19 Eval examples.\n",
            "\n",
            "Copying traning files from ./dataset/basic/meme to ./dataset/train/no_momo\n",
            "100%|██████████| 91/91 [00:00<00:00, 2614.42it/s]\n",
            "Copying testing files from ./dataset/basic/meme to ./dataset/test/no_momo\n",
            "100%|██████████| 20/20 [00:00<00:00, 2118.60it/s]\n",
            "Copying eval files from ./dataset/basic/meme to ./dataset/eval/no_momo\n",
            "100%|██████████| 19/19 [00:00<00:00, 2207.90it/s]\n",
            "\n",
            "\n",
            "From folder 'landscapes' take 53 Training examples, 12 Testing examples, and 10 Eval examples.\n",
            "\n",
            "Copying traning files from ./dataset/basic/landscapes to ./dataset/train/no_momo\n",
            "100%|██████████| 53/53 [00:00<00:00, 2261.33it/s]\n",
            "Copying testing files from ./dataset/basic/landscapes to ./dataset/test/no_momo\n",
            "100%|██████████| 12/12 [00:00<00:00, 1523.54it/s]\n",
            "Copying eval files from ./dataset/basic/landscapes to ./dataset/eval/no_momo\n",
            "100%|██████████| 10/10 [00:00<00:00, 1686.49it/s]\n",
            "\n",
            "\n",
            "From folder 'scary' take 61 Training examples, 13 Testing examples, and 12 Eval examples.\n",
            "\n",
            "Copying traning files from ./dataset/basic/scary to ./dataset/train/no_momo\n",
            "100%|██████████| 61/61 [00:00<00:00, 2911.58it/s]\n",
            "Copying testing files from ./dataset/basic/scary to ./dataset/test/no_momo\n",
            "100%|██████████| 13/13 [00:00<00:00, 1730.76it/s]\n",
            "Copying eval files from ./dataset/basic/scary to ./dataset/eval/no_momo\n",
            "100%|██████████| 12/12 [00:00<00:00, 1990.42it/s]\n",
            "\n",
            "\n",
            "From folder 'montains' take 59 Training examples, 13 Testing examples, and 12 Eval examples.\n",
            "\n",
            "Copying traning files from ./dataset/basic/montains to ./dataset/train/no_momo\n",
            "100%|██████████| 59/59 [00:00<00:00, 1796.72it/s]\n",
            "Copying testing files from ./dataset/basic/montains to ./dataset/test/no_momo\n",
            "100%|██████████| 13/13 [00:00<00:00, 1346.45it/s]\n",
            "Copying eval files from ./dataset/basic/montains to ./dataset/eval/no_momo\n",
            "100%|██████████| 12/12 [00:00<00:00, 931.00it/s]\n",
            "\n",
            "\n",
            "From folder 'momo' take 304 Training examples, 66 Testing examples, and 64 Eval examples.\n",
            "\n",
            "Copying traning files from ./dataset/basic/momo to ./dataset/train/momo\n",
            "100%|██████████| 304/304 [00:00<00:00, 4202.09it/s]\n",
            "Copying testing files from ./dataset/basic/momo to ./dataset/test/momo\n",
            "100%|██████████| 66/66 [00:00<00:00, 2678.85it/s]\n",
            "Copying eval files from ./dataset/basic/momo to ./dataset/eval/momo\n",
            "100%|██████████| 64/64 [00:00<00:00, 2016.27it/s]\n",
            "\n",
            "\n",
            "From folder 'person' take 130 Training examples, 28 Testing examples, and 27 Eval examples.\n",
            "\n",
            "Copying traning files from ./dataset/basic/person to ./dataset/train/no_momo\n",
            "100%|██████████| 130/130 [00:00<00:00, 1402.96it/s]\n",
            "Copying testing files from ./dataset/basic/person to ./dataset/test/no_momo\n",
            "100%|██████████| 28/28 [00:00<00:00, 2437.08it/s]\n",
            "Copying eval files from ./dataset/basic/person to ./dataset/eval/no_momo\n",
            "100%|██████████| 27/27 [00:00<00:00, 1177.94it/s]\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KmByjHODA9Av"
      },
      "source": [
        "\n",
        "## Data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jXMsHscUA9Av",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0f12550b-28b9-454b-879d-e4501eaa2277"
      },
      "source": [
        "# Create a generator to pre process our dataset images\n",
        "imageGenerator = ImageDataGenerator(\n",
        "    rescale            = 1./255,       # Scale our data to our dataset scale\n",
        "    horizontal_flip    = True,         # Horizontal mirror\n",
        "    vertical_flip      = False,         # Disable vertical mirror\n",
        "    rotation_range     = 7,\n",
        "    width_shift_range  = 0.3,\n",
        "    height_shift_range = 0.3,\n",
        "    brightness_range   = (0.3, 0.7),\n",
        "    shear_range        = 5.0,\n",
        "    fill_mode          = \"reflect\"\n",
        ")\n",
        "\n",
        "trainGenerator = imageGenerator.flow_from_directory(\n",
        "        directory   = DATESET_TRAINING_PATH,\n",
        "        batch_size  =  BATCH_SIZE,\n",
        "        class_mode  =  'binary',\n",
        "        classes     =  getOutputClasses())\n",
        "\n",
        "testGenerator= imageGenerator.flow_from_directory(\n",
        "        directory    = DATESET_TESTING_PATH,\n",
        "\n",
        "        batch_size   = BATCH_SIZE,\n",
        "        class_mode   = 'binary',\n",
        "        classes      = getOutputClasses())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 434 images belonging to 2 classes.\n",
            "Found 94 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5fa2xYh4A9Ax",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "491a84e8-0895-4525-e324-c4092de75d21"
      },
      "source": [
        "trainGenerator.class_indices"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'momo': 1, 'no_momo': 0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-PvUAm4lA9Az"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3_g4RiH_A9A0"
      },
      "source": [
        "Lets save some util data from our foulders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c8X7Jyp3A9A0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "54d8de3e-4951-45f7-8ebd-bbadbc8963fc"
      },
      "source": [
        "trainPositiveFiles          = getFolderFiles(DATESET_TRAINING_PATH + MOMO_CLASSNAME)\n",
        "trainNegativeFiles          = getFolderFiles(DATESET_TRAINING_PATH + NO_MOMO_CLASSNAME)\n",
        "trainPositiveExamplesLength = len(trainPositiveFiles)\n",
        "trainNegativeExamplesLength = len(trainNegativeFiles)\n",
        "trainLength                 = trainPositiveExamplesLength + trainNegativeExamplesLength\n",
        "\n",
        "print(\"Total train examples: \", trainLength , \"positive: \" , trainPositiveExamplesLength  , \" negative: \" , trainNegativeExamplesLength)\n",
        "\n",
        "testPositiveFiles           = getFolderFiles(DATESET_TESTING_PATH + MOMO_CLASSNAME)\n",
        "testNegativeFiles           = getFolderFiles(DATESET_TESTING_PATH + NO_MOMO_CLASSNAME)\n",
        "testPositiveExamplesLength  = len(testPositiveFiles)\n",
        "testNegativeExamplesLength  = len(testNegativeFiles)\n",
        "testLength                  = testPositiveExamplesLength + testNegativeExamplesLength\n",
        "\n",
        "print(\"Total test examples: \", testLength , \"positive: \" , testPositiveExamplesLength  , \" negative: \" , testNegativeExamplesLength)\n",
        "print()\n",
        "\n",
        "trainSteps = trainLength // BATCH_SIZE\n",
        "testSteps  = testLength // BATCH_SIZE\n",
        "\n",
        "print(\"Train Steps per epoch \" , trainSteps)\n",
        "print(\"Test Steps per epoch \" , testSteps)\n",
        "\n",
        "\n",
        "resetFolderIfExist(RESULT_FOLDER_WEIGHTS_INCEPTION_V3_PATH)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total train examples:  434 positive:  304  negative:  130\n",
            "Total test examples:  94 positive:  66  negative:  28\n",
            "\n",
            "Train Steps per epoch  28\n",
            "Test Steps per epoch  6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1fW7oLLTA9A2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "ec9bf9e7-2bd4-4e69-f7ea-58a1c905c780"
      },
      "source": [
        "TRAINING_TIME_START = time()\n",
        "progress = momoModel.fit_generator(\n",
        "        trainGenerator,\n",
        "        steps_per_epoch=trainSteps,\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=testGenerator,\n",
        "        validation_steps=testSteps\n",
        ")\n",
        "TRAINING_TIME_END = time()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-14-28c677e904e1>:7: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/10\n",
            "28/28 [==============================] - 18s 654ms/step - loss: 0.6490 - accuracy: 0.8067 - val_loss: 0.0572 - val_accuracy: 0.9889\n",
            "Epoch 2/10\n",
            "28/28 [==============================] - 16s 577ms/step - loss: 0.1010 - accuracy: 0.9571 - val_loss: 0.2305 - val_accuracy: 0.9111\n",
            "Epoch 3/10\n",
            "28/28 [==============================] - 16s 578ms/step - loss: 0.0824 - accuracy: 0.9737 - val_loss: 0.0518 - val_accuracy: 0.9778\n",
            "Epoch 4/10\n",
            "28/28 [==============================] - 16s 573ms/step - loss: 0.1081 - accuracy: 0.9618 - val_loss: 0.1695 - val_accuracy: 0.9222\n",
            "Epoch 5/10\n",
            "28/28 [==============================] - 16s 580ms/step - loss: 0.0858 - accuracy: 0.9666 - val_loss: 0.0639 - val_accuracy: 0.9667\n",
            "Epoch 6/10\n",
            "28/28 [==============================] - 16s 582ms/step - loss: 0.0318 - accuracy: 0.9881 - val_loss: 0.0502 - val_accuracy: 0.9778\n",
            "Epoch 7/10\n",
            "28/28 [==============================] - 16s 582ms/step - loss: 0.0711 - accuracy: 0.9833 - val_loss: 0.1071 - val_accuracy: 0.9556\n",
            "Epoch 8/10\n",
            "28/28 [==============================] - 16s 578ms/step - loss: 0.0782 - accuracy: 0.9737 - val_loss: 0.0318 - val_accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "28/28 [==============================] - 16s 580ms/step - loss: 0.0618 - accuracy: 0.9785 - val_loss: 0.0489 - val_accuracy: 0.9889\n",
            "Epoch 10/10\n",
            "28/28 [==============================] - 16s 562ms/step - loss: 0.0254 - accuracy: 0.9928 - val_loss: 0.0165 - val_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sq9RjGn4A9A8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "outputId": "6db53fe7-4a88-4dba-ea65-6ce80a906300"
      },
      "source": [
        "def saveInceptionV3CheckpointAndDraw():\n",
        "\n",
        "  print(progress.history.keys())\n",
        "  loss              = progress.history[\"loss\"][-1]\n",
        "  val_loss          = progress.history[\"val_loss\"][-1]\n",
        "\n",
        "  accuracy         = progress.history[\"accuracy\"][-1]\n",
        "  val_accuracy     = progress.history[\"val_accuracy\"][-1]\n",
        "\n",
        "  perfData = {\n",
        "        'loss'         :  loss,             \n",
        "        'val_loss'     :  val_loss,    \n",
        "        'accuracy'     :  accuracy,    \n",
        "        'val_accuracy' :  val_accuracy,\n",
        "        'training_time':  (TRAINING_TIME_END - TRAINING_TIME_START),\n",
        "        'train_length' :  trainLength,\n",
        "        'test_length'  :  testLength\n",
        "  }\n",
        "\n",
        "  \n",
        "  momoModel.save_weights( DEFAULT_WEIGHTS_FILE_PATH)\n",
        "  plotModelTrainingProgress(progress)\n",
        "  return perfData \n",
        "\n",
        "perfData = saveInceptionV3CheckpointAndDraw()\n",
        "performanceDF = performanceDF.append(perfData, ignore_index=True)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0OnY0qoXA9A_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "c2858d01-f7ee-435b-e13c-a22819ad73c0"
      },
      "source": [
        "printConfussionMatrix(momoModel)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing no_momo  total of images:  27\n",
            "100%|██████████| 27/27 [00:04<00:00,  5.95it/s]\n",
            "Processing momo  total of images:  64\n",
            "100%|██████████| 64/64 [00:05<00:00, 11.56it/s]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICuaFnlPVkNk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "879513e2-1aaf-4180-cbac-d753e5fe88ab"
      },
      "source": [
        "# View performance data\n",
        "performanceDF.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>test_length</th>\n",
              "      <th>train_length</th>\n",
              "      <th>training_time</th>\n",
              "      <th>val_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.99284</td>\n",
              "      <td>0.025446</td>\n",
              "      <td>94.0</td>\n",
              "      <td>434.0</td>\n",
              "      <td>176.843293</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.016534</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   accuracy      loss  test_length  ...  training_time  val_accuracy  val_loss\n",
              "0   0.99284  0.025446         94.0  ...     176.843293           1.0  0.016534\n",
              "\n",
              "[1 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95PehCE5pBQs",
        "colab_type": "text"
      },
      "source": [
        "# Improving training time using a bottleneck model\n",
        "\n",
        "Our model generalizes well, however this approach takes 15 seconds (aprox depending on your environmen) per epoch. We can improving this to 1 or less per sec. However, it will cost to us loss on train/testing set\n",
        "\n",
        "First, we have all the embdings of our dataset in the follow at:\n",
        "\n",
        "`/content/result/basic-predictions/latest/results.tsv`\n",
        "\n",
        "This file contains all the 2048 embdings  generated by Inception V3. That data was pulled from the visualization repo. \n",
        "\n",
        "Lets do the next:\n",
        "\n",
        "1. Read the embdings.\n",
        "2. Read the metadata.\n",
        "3. Build a new datasets where the input are the embdings and Y is [0,1] if its momo or not.\n",
        "4. Build a bottleneck NN that receive the embdings and out the probability of be momo or not\n",
        "\n",
        "*We need to be carefull because the embding files contains all the dataset, so we need to split the data in train, test and eval*\n",
        "\n",
        "For this reason we need to use the `getDistribuitions` function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJntqhsxGH9Z",
        "colab_type": "text"
      },
      "source": [
        "# Train using the embdings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6DUDvpTul8U",
        "colab_type": "text"
      },
      "source": [
        "## Create dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66Frw6Tzusol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resultsDF = pd.read_csv('result/basic-predictions/latest/result.csv')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EUffzm_vG3V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "091a25bb-0212-4197-8ad6-d3b58f704f6b"
      },
      "source": [
        "resultsDF.head()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>image_name</th>\n",
              "      <th>predictions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>landscapes</td>\n",
              "      <td>night_landscapes_image69.jpg</td>\n",
              "      <td>0.18665837,0.2011668,0.27758992,0.4505339,0.67...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>landscapes</td>\n",
              "      <td>night_landscapes_image82.jpg</td>\n",
              "      <td>0.121671416,0.33406463,0.18043913,0.037964597,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>landscapes</td>\n",
              "      <td>night_landscapes_image1.jpg</td>\n",
              "      <td>0.09562505,0.18875906,0.10296103,0.40914068,0....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>landscapes</td>\n",
              "      <td>night_landscapes_image97.jpg</td>\n",
              "      <td>0.19299471,0.44939288,0.27516326,0.27659455,0....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>landscapes</td>\n",
              "      <td>night_landscapes_image54.jpg</td>\n",
              "      <td>0.12543976,0.3352605,0.18066786,0.22412151,0.3...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        class  ...                                        predictions\n",
              "0  landscapes  ...  0.18665837,0.2011668,0.27758992,0.4505339,0.67...\n",
              "1  landscapes  ...  0.121671416,0.33406463,0.18043913,0.037964597,...\n",
              "2  landscapes  ...  0.09562505,0.18875906,0.10296103,0.40914068,0....\n",
              "3  landscapes  ...  0.19299471,0.44939288,0.27516326,0.27659455,0....\n",
              "4  landscapes  ...  0.12543976,0.3352605,0.18066786,0.22412151,0.3...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiC-wroWxt5c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "9d295626-caf8-43ed-d705-8bc3b1e8d643"
      },
      "source": [
        "# Show where records are momo\n",
        "resultsDF[resultsDF[\"class\"].eq(\"momo\")]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>image_name</th>\n",
              "      <th>predictions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>476</th>\n",
              "      <td>momo</td>\n",
              "      <td>momo_image60.jpg</td>\n",
              "      <td>0.24718286,0.32045782,0.45376766,0.029030561,0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>477</th>\n",
              "      <td>momo</td>\n",
              "      <td>momo_image48.jpg</td>\n",
              "      <td>0.91144085,1.1288024,0.27570406,0.35434362,0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>478</th>\n",
              "      <td>momo</td>\n",
              "      <td>momo_noticiasimage33.jpg</td>\n",
              "      <td>0.11241783,0.59009475,0.2327288,0.12253951,0.7...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>479</th>\n",
              "      <td>momo</td>\n",
              "      <td>momo_terror_image13.jpg</td>\n",
              "      <td>0.6961774,0.86684567,0.48696104,0.233787,0.345...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>480</th>\n",
              "      <td>momo</td>\n",
              "      <td>momo_videoimage21.jpg</td>\n",
              "      <td>0.867249,0.6748508,0.28455222,0.10471753,0.336...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>905</th>\n",
              "      <td>momo</td>\n",
              "      <td>momo_terror_image36.jpg</td>\n",
              "      <td>0.61178976,0.12942956,0.27977118,0.4380286,0.1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>906</th>\n",
              "      <td>momo</td>\n",
              "      <td>momo_terror_image22.jpg</td>\n",
              "      <td>0.48330578,0.38772425,0.086077705,0.115843534,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>907</th>\n",
              "      <td>momo</td>\n",
              "      <td>momo_image45.jpg</td>\n",
              "      <td>0.41436443,0.6651123,0.35841405,1.2633588,0.03...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>908</th>\n",
              "      <td>momo</td>\n",
              "      <td>momo_historiaimage60.jpg</td>\n",
              "      <td>0.108510874,0.2925446,0.022094179,0.41120535,0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>909</th>\n",
              "      <td>momo</td>\n",
              "      <td>momo_historiaimage48.jpg</td>\n",
              "      <td>0.35079038,0.6675016,0.41968906,0.3541645,0.23...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>434 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    class  ...                                        predictions\n",
              "476  momo  ...  0.24718286,0.32045782,0.45376766,0.029030561,0...\n",
              "477  momo  ...  0.91144085,1.1288024,0.27570406,0.35434362,0.0...\n",
              "478  momo  ...  0.11241783,0.59009475,0.2327288,0.12253951,0.7...\n",
              "479  momo  ...  0.6961774,0.86684567,0.48696104,0.233787,0.345...\n",
              "480  momo  ...  0.867249,0.6748508,0.28455222,0.10471753,0.336...\n",
              "..    ...  ...                                                ...\n",
              "905  momo  ...  0.61178976,0.12942956,0.27977118,0.4380286,0.1...\n",
              "906  momo  ...  0.48330578,0.38772425,0.086077705,0.115843534,...\n",
              "907  momo  ...  0.41436443,0.6651123,0.35841405,1.2633588,0.03...\n",
              "908  momo  ...  0.108510874,0.2925446,0.022094179,0.41120535,0...\n",
              "909  momo  ...  0.35079038,0.6675016,0.41968906,0.3541645,0.23...\n",
              "\n",
              "[434 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1Sfy5Raylwf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fd8c18f8-e283-41f4-dd24-ef186b02e0b3"
      },
      "source": [
        "# Take the Y array  and compare with the data above\n",
        "resultsY = resultsDF[\"class\"] == \"momo\"\n",
        "resultsY = resultsY.to_list()\n",
        "\n",
        "\n",
        "#lets verify the index (only for demonstration purposes)\n",
        "isMomoArray = resultsDF[\"class\"] == \"momo\"\n",
        "index_error = False\n",
        "for i in range(0,len(resultsDF[\"class\"])):\n",
        "  isMomo = isMomoArray[i]\n",
        "  y_i    = resultsY[i]\n",
        "\n",
        "  if  y_i !=  isMomo:\n",
        "    index_error = True\n",
        "    print(\"error\",i)\n",
        "\n",
        "if index_error :\n",
        "  print(\"Error in indices\")\n",
        "else: \n",
        "  print(\"It is ok, all indices matchs each other\")\n",
        "\n",
        "print(\"rows:\" , len(resultsDF[\"class\"] == \"momo\"))\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "It is ok, all indices matchs each other\n",
            "rows: 994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sj8NLxOu1Zb6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "69f4d124-4c40-4510-ddb4-08e547a2166b"
      },
      "source": [
        "resultsY = [1 if result == True or result == 1 else 0 for result in resultsY] \n",
        "print(\"rows:\" , len(resultsY))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rows: 994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VBiqQ4552K5",
        "colab_type": "text"
      },
      "source": [
        "## Get distributions from dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fO6wcbcT6K6M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "b1f6fe2c-b63b-4e87-8836-967d077b6f9f"
      },
      "source": [
        "# Get the predictions as float\n",
        "\n",
        "arrayOfFeatures = resultsDF['predictions'].apply(lambda x: np.array([float(str_dim) for str_dim in x.split(',')], dtype=np.float32)).to_numpy().tolist()\n",
        "embdings = pd.DataFrame(arrayOfFeatures)\n",
        "\n",
        "embdings.head(10)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>2008</th>\n",
              "      <th>2009</th>\n",
              "      <th>2010</th>\n",
              "      <th>2011</th>\n",
              "      <th>2012</th>\n",
              "      <th>2013</th>\n",
              "      <th>2014</th>\n",
              "      <th>2015</th>\n",
              "      <th>2016</th>\n",
              "      <th>2017</th>\n",
              "      <th>2018</th>\n",
              "      <th>2019</th>\n",
              "      <th>2020</th>\n",
              "      <th>2021</th>\n",
              "      <th>2022</th>\n",
              "      <th>2023</th>\n",
              "      <th>2024</th>\n",
              "      <th>2025</th>\n",
              "      <th>2026</th>\n",
              "      <th>2027</th>\n",
              "      <th>2028</th>\n",
              "      <th>2029</th>\n",
              "      <th>2030</th>\n",
              "      <th>2031</th>\n",
              "      <th>2032</th>\n",
              "      <th>2033</th>\n",
              "      <th>2034</th>\n",
              "      <th>2035</th>\n",
              "      <th>2036</th>\n",
              "      <th>2037</th>\n",
              "      <th>2038</th>\n",
              "      <th>2039</th>\n",
              "      <th>2040</th>\n",
              "      <th>2041</th>\n",
              "      <th>2042</th>\n",
              "      <th>2043</th>\n",
              "      <th>2044</th>\n",
              "      <th>2045</th>\n",
              "      <th>2046</th>\n",
              "      <th>2047</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.186658</td>\n",
              "      <td>0.201167</td>\n",
              "      <td>0.277590</td>\n",
              "      <td>0.450534</td>\n",
              "      <td>0.679752</td>\n",
              "      <td>0.557190</td>\n",
              "      <td>0.290200</td>\n",
              "      <td>0.049228</td>\n",
              "      <td>0.003357</td>\n",
              "      <td>0.435891</td>\n",
              "      <td>0.233772</td>\n",
              "      <td>0.372941</td>\n",
              "      <td>0.018152</td>\n",
              "      <td>0.220918</td>\n",
              "      <td>0.059483</td>\n",
              "      <td>0.287004</td>\n",
              "      <td>0.349816</td>\n",
              "      <td>0.399400</td>\n",
              "      <td>0.119716</td>\n",
              "      <td>0.076740</td>\n",
              "      <td>0.150059</td>\n",
              "      <td>0.081949</td>\n",
              "      <td>0.359989</td>\n",
              "      <td>0.139433</td>\n",
              "      <td>0.217959</td>\n",
              "      <td>0.141277</td>\n",
              "      <td>0.034335</td>\n",
              "      <td>0.325931</td>\n",
              "      <td>0.337279</td>\n",
              "      <td>1.040228</td>\n",
              "      <td>0.449910</td>\n",
              "      <td>0.664118</td>\n",
              "      <td>0.186733</td>\n",
              "      <td>0.202194</td>\n",
              "      <td>0.178565</td>\n",
              "      <td>0.343330</td>\n",
              "      <td>0.091849</td>\n",
              "      <td>0.120142</td>\n",
              "      <td>0.855181</td>\n",
              "      <td>0.431540</td>\n",
              "      <td>...</td>\n",
              "      <td>0.303280</td>\n",
              "      <td>0.412272</td>\n",
              "      <td>0.837927</td>\n",
              "      <td>0.451757</td>\n",
              "      <td>0.853336</td>\n",
              "      <td>0.359832</td>\n",
              "      <td>1.625067</td>\n",
              "      <td>0.369905</td>\n",
              "      <td>0.109124</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.011018</td>\n",
              "      <td>0.509020</td>\n",
              "      <td>0.202400</td>\n",
              "      <td>0.532138</td>\n",
              "      <td>1.187709</td>\n",
              "      <td>0.011353</td>\n",
              "      <td>0.586955</td>\n",
              "      <td>0.484805</td>\n",
              "      <td>1.180369</td>\n",
              "      <td>0.171564</td>\n",
              "      <td>1.113664</td>\n",
              "      <td>0.577205</td>\n",
              "      <td>1.752373</td>\n",
              "      <td>0.423139</td>\n",
              "      <td>0.434294</td>\n",
              "      <td>0.013892</td>\n",
              "      <td>0.021528</td>\n",
              "      <td>0.819288</td>\n",
              "      <td>0.197840</td>\n",
              "      <td>1.136222</td>\n",
              "      <td>0.380921</td>\n",
              "      <td>0.814568</td>\n",
              "      <td>0.853492</td>\n",
              "      <td>1.155724</td>\n",
              "      <td>0.020632</td>\n",
              "      <td>0.197873</td>\n",
              "      <td>0.199785</td>\n",
              "      <td>0.090156</td>\n",
              "      <td>0.106479</td>\n",
              "      <td>0.198679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.121671</td>\n",
              "      <td>0.334065</td>\n",
              "      <td>0.180439</td>\n",
              "      <td>0.037965</td>\n",
              "      <td>0.702711</td>\n",
              "      <td>0.480539</td>\n",
              "      <td>0.405303</td>\n",
              "      <td>0.106590</td>\n",
              "      <td>0.133642</td>\n",
              "      <td>0.714266</td>\n",
              "      <td>0.034964</td>\n",
              "      <td>0.097646</td>\n",
              "      <td>0.683623</td>\n",
              "      <td>0.259163</td>\n",
              "      <td>0.812722</td>\n",
              "      <td>0.273802</td>\n",
              "      <td>0.393268</td>\n",
              "      <td>0.832693</td>\n",
              "      <td>0.411869</td>\n",
              "      <td>1.396989</td>\n",
              "      <td>0.234263</td>\n",
              "      <td>0.211668</td>\n",
              "      <td>0.617950</td>\n",
              "      <td>0.065147</td>\n",
              "      <td>0.033820</td>\n",
              "      <td>0.138741</td>\n",
              "      <td>0.840643</td>\n",
              "      <td>0.389924</td>\n",
              "      <td>0.236197</td>\n",
              "      <td>0.297788</td>\n",
              "      <td>0.797041</td>\n",
              "      <td>0.263471</td>\n",
              "      <td>0.149280</td>\n",
              "      <td>0.091979</td>\n",
              "      <td>0.666626</td>\n",
              "      <td>0.170802</td>\n",
              "      <td>0.060599</td>\n",
              "      <td>0.645227</td>\n",
              "      <td>0.024508</td>\n",
              "      <td>0.222306</td>\n",
              "      <td>...</td>\n",
              "      <td>0.474003</td>\n",
              "      <td>0.326125</td>\n",
              "      <td>0.633693</td>\n",
              "      <td>0.259358</td>\n",
              "      <td>0.070400</td>\n",
              "      <td>0.185912</td>\n",
              "      <td>1.392322</td>\n",
              "      <td>0.150397</td>\n",
              "      <td>0.128107</td>\n",
              "      <td>0.273685</td>\n",
              "      <td>0.265904</td>\n",
              "      <td>1.109848</td>\n",
              "      <td>0.171083</td>\n",
              "      <td>1.015648</td>\n",
              "      <td>0.053169</td>\n",
              "      <td>0.048087</td>\n",
              "      <td>0.206891</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.372582</td>\n",
              "      <td>0.720872</td>\n",
              "      <td>1.093609</td>\n",
              "      <td>0.060636</td>\n",
              "      <td>0.228902</td>\n",
              "      <td>0.055357</td>\n",
              "      <td>0.236972</td>\n",
              "      <td>0.415477</td>\n",
              "      <td>0.271191</td>\n",
              "      <td>0.219446</td>\n",
              "      <td>0.020738</td>\n",
              "      <td>1.132337</td>\n",
              "      <td>0.092776</td>\n",
              "      <td>0.179989</td>\n",
              "      <td>0.186551</td>\n",
              "      <td>0.491712</td>\n",
              "      <td>0.005300</td>\n",
              "      <td>0.017488</td>\n",
              "      <td>0.081028</td>\n",
              "      <td>0.116354</td>\n",
              "      <td>0.276918</td>\n",
              "      <td>0.309094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.095625</td>\n",
              "      <td>0.188759</td>\n",
              "      <td>0.102961</td>\n",
              "      <td>0.409141</td>\n",
              "      <td>0.233870</td>\n",
              "      <td>0.178092</td>\n",
              "      <td>0.144872</td>\n",
              "      <td>0.684785</td>\n",
              "      <td>0.030607</td>\n",
              "      <td>0.205525</td>\n",
              "      <td>0.146331</td>\n",
              "      <td>0.326769</td>\n",
              "      <td>0.159326</td>\n",
              "      <td>0.340099</td>\n",
              "      <td>0.056878</td>\n",
              "      <td>0.600987</td>\n",
              "      <td>0.525919</td>\n",
              "      <td>0.425087</td>\n",
              "      <td>0.020922</td>\n",
              "      <td>0.267537</td>\n",
              "      <td>0.420028</td>\n",
              "      <td>0.051148</td>\n",
              "      <td>0.512424</td>\n",
              "      <td>0.032861</td>\n",
              "      <td>0.012830</td>\n",
              "      <td>0.731387</td>\n",
              "      <td>0.179493</td>\n",
              "      <td>0.481331</td>\n",
              "      <td>0.321772</td>\n",
              "      <td>0.481045</td>\n",
              "      <td>0.908872</td>\n",
              "      <td>0.135501</td>\n",
              "      <td>0.348812</td>\n",
              "      <td>0.052072</td>\n",
              "      <td>0.593057</td>\n",
              "      <td>0.310557</td>\n",
              "      <td>0.058454</td>\n",
              "      <td>0.270207</td>\n",
              "      <td>1.275407</td>\n",
              "      <td>0.155891</td>\n",
              "      <td>...</td>\n",
              "      <td>1.139673</td>\n",
              "      <td>0.615175</td>\n",
              "      <td>0.342320</td>\n",
              "      <td>0.121017</td>\n",
              "      <td>0.098823</td>\n",
              "      <td>0.789064</td>\n",
              "      <td>0.793131</td>\n",
              "      <td>0.181145</td>\n",
              "      <td>0.855039</td>\n",
              "      <td>0.020278</td>\n",
              "      <td>0.234760</td>\n",
              "      <td>0.117423</td>\n",
              "      <td>0.152047</td>\n",
              "      <td>0.897394</td>\n",
              "      <td>0.705481</td>\n",
              "      <td>0.405510</td>\n",
              "      <td>1.027417</td>\n",
              "      <td>0.164821</td>\n",
              "      <td>0.420691</td>\n",
              "      <td>0.494159</td>\n",
              "      <td>2.085188</td>\n",
              "      <td>1.124250</td>\n",
              "      <td>1.005321</td>\n",
              "      <td>0.799797</td>\n",
              "      <td>0.809275</td>\n",
              "      <td>0.003129</td>\n",
              "      <td>0.560752</td>\n",
              "      <td>0.195829</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.290465</td>\n",
              "      <td>0.467819</td>\n",
              "      <td>0.025492</td>\n",
              "      <td>0.119554</td>\n",
              "      <td>2.963231</td>\n",
              "      <td>0.658304</td>\n",
              "      <td>0.136560</td>\n",
              "      <td>0.372603</td>\n",
              "      <td>0.064262</td>\n",
              "      <td>0.178026</td>\n",
              "      <td>1.056082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.192995</td>\n",
              "      <td>0.449393</td>\n",
              "      <td>0.275163</td>\n",
              "      <td>0.276595</td>\n",
              "      <td>0.104226</td>\n",
              "      <td>0.313887</td>\n",
              "      <td>0.150739</td>\n",
              "      <td>0.241162</td>\n",
              "      <td>0.106343</td>\n",
              "      <td>0.232645</td>\n",
              "      <td>0.020406</td>\n",
              "      <td>0.117919</td>\n",
              "      <td>0.386289</td>\n",
              "      <td>0.146838</td>\n",
              "      <td>0.164870</td>\n",
              "      <td>0.582048</td>\n",
              "      <td>0.229578</td>\n",
              "      <td>0.226784</td>\n",
              "      <td>0.106215</td>\n",
              "      <td>0.335209</td>\n",
              "      <td>0.297369</td>\n",
              "      <td>0.020923</td>\n",
              "      <td>0.486241</td>\n",
              "      <td>0.090963</td>\n",
              "      <td>0.016745</td>\n",
              "      <td>0.490046</td>\n",
              "      <td>0.152940</td>\n",
              "      <td>0.396862</td>\n",
              "      <td>0.224672</td>\n",
              "      <td>1.045330</td>\n",
              "      <td>0.585388</td>\n",
              "      <td>0.186552</td>\n",
              "      <td>0.256583</td>\n",
              "      <td>0.075409</td>\n",
              "      <td>0.419912</td>\n",
              "      <td>0.400853</td>\n",
              "      <td>0.066843</td>\n",
              "      <td>0.069252</td>\n",
              "      <td>0.257802</td>\n",
              "      <td>0.144355</td>\n",
              "      <td>...</td>\n",
              "      <td>0.235505</td>\n",
              "      <td>0.545300</td>\n",
              "      <td>0.169635</td>\n",
              "      <td>0.596891</td>\n",
              "      <td>0.001234</td>\n",
              "      <td>0.034026</td>\n",
              "      <td>0.249636</td>\n",
              "      <td>0.024552</td>\n",
              "      <td>0.406844</td>\n",
              "      <td>0.339856</td>\n",
              "      <td>0.024830</td>\n",
              "      <td>0.953234</td>\n",
              "      <td>0.140756</td>\n",
              "      <td>0.223561</td>\n",
              "      <td>0.706568</td>\n",
              "      <td>0.850970</td>\n",
              "      <td>0.294396</td>\n",
              "      <td>0.081041</td>\n",
              "      <td>0.165759</td>\n",
              "      <td>0.282878</td>\n",
              "      <td>1.513223</td>\n",
              "      <td>0.151811</td>\n",
              "      <td>1.238532</td>\n",
              "      <td>0.511704</td>\n",
              "      <td>0.006007</td>\n",
              "      <td>0.269234</td>\n",
              "      <td>1.316672</td>\n",
              "      <td>0.011098</td>\n",
              "      <td>0.000804</td>\n",
              "      <td>1.006434</td>\n",
              "      <td>0.015888</td>\n",
              "      <td>0.165658</td>\n",
              "      <td>0.357495</td>\n",
              "      <td>1.568861</td>\n",
              "      <td>0.895973</td>\n",
              "      <td>0.219709</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.086952</td>\n",
              "      <td>0.285739</td>\n",
              "      <td>0.036129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.125440</td>\n",
              "      <td>0.335261</td>\n",
              "      <td>0.180668</td>\n",
              "      <td>0.224122</td>\n",
              "      <td>0.387346</td>\n",
              "      <td>0.053199</td>\n",
              "      <td>0.004590</td>\n",
              "      <td>0.341848</td>\n",
              "      <td>0.001810</td>\n",
              "      <td>0.477891</td>\n",
              "      <td>0.013860</td>\n",
              "      <td>0.488438</td>\n",
              "      <td>0.246895</td>\n",
              "      <td>0.413584</td>\n",
              "      <td>0.205500</td>\n",
              "      <td>0.345214</td>\n",
              "      <td>0.205500</td>\n",
              "      <td>0.383287</td>\n",
              "      <td>0.030998</td>\n",
              "      <td>0.148177</td>\n",
              "      <td>0.100476</td>\n",
              "      <td>0.129665</td>\n",
              "      <td>0.661583</td>\n",
              "      <td>0.426572</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.677275</td>\n",
              "      <td>0.191681</td>\n",
              "      <td>0.621816</td>\n",
              "      <td>0.333985</td>\n",
              "      <td>0.516549</td>\n",
              "      <td>0.591228</td>\n",
              "      <td>0.380218</td>\n",
              "      <td>0.038230</td>\n",
              "      <td>0.086566</td>\n",
              "      <td>0.424608</td>\n",
              "      <td>0.429468</td>\n",
              "      <td>0.039960</td>\n",
              "      <td>0.129232</td>\n",
              "      <td>0.548597</td>\n",
              "      <td>0.071301</td>\n",
              "      <td>...</td>\n",
              "      <td>0.888134</td>\n",
              "      <td>0.227684</td>\n",
              "      <td>0.585849</td>\n",
              "      <td>0.555268</td>\n",
              "      <td>0.279131</td>\n",
              "      <td>0.004949</td>\n",
              "      <td>0.904657</td>\n",
              "      <td>0.011038</td>\n",
              "      <td>0.158753</td>\n",
              "      <td>0.399309</td>\n",
              "      <td>0.071014</td>\n",
              "      <td>0.659698</td>\n",
              "      <td>0.102262</td>\n",
              "      <td>1.030287</td>\n",
              "      <td>0.581589</td>\n",
              "      <td>0.084956</td>\n",
              "      <td>0.580619</td>\n",
              "      <td>0.118928</td>\n",
              "      <td>0.232220</td>\n",
              "      <td>0.734922</td>\n",
              "      <td>0.530656</td>\n",
              "      <td>0.064879</td>\n",
              "      <td>0.189440</td>\n",
              "      <td>0.267907</td>\n",
              "      <td>0.035129</td>\n",
              "      <td>0.073167</td>\n",
              "      <td>0.296866</td>\n",
              "      <td>0.205544</td>\n",
              "      <td>0.025095</td>\n",
              "      <td>0.959758</td>\n",
              "      <td>0.027288</td>\n",
              "      <td>0.089680</td>\n",
              "      <td>0.260602</td>\n",
              "      <td>0.787471</td>\n",
              "      <td>0.567632</td>\n",
              "      <td>0.152253</td>\n",
              "      <td>0.021113</td>\n",
              "      <td>0.000587</td>\n",
              "      <td>0.710166</td>\n",
              "      <td>0.512944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.032018</td>\n",
              "      <td>0.024085</td>\n",
              "      <td>0.264659</td>\n",
              "      <td>0.041862</td>\n",
              "      <td>0.695062</td>\n",
              "      <td>0.204238</td>\n",
              "      <td>0.259592</td>\n",
              "      <td>0.164488</td>\n",
              "      <td>0.058913</td>\n",
              "      <td>0.587188</td>\n",
              "      <td>0.127170</td>\n",
              "      <td>0.425489</td>\n",
              "      <td>0.019610</td>\n",
              "      <td>0.443134</td>\n",
              "      <td>0.119211</td>\n",
              "      <td>0.797235</td>\n",
              "      <td>0.172987</td>\n",
              "      <td>0.114630</td>\n",
              "      <td>0.011167</td>\n",
              "      <td>0.183958</td>\n",
              "      <td>0.360305</td>\n",
              "      <td>0.892831</td>\n",
              "      <td>0.043099</td>\n",
              "      <td>0.884332</td>\n",
              "      <td>0.003610</td>\n",
              "      <td>0.092358</td>\n",
              "      <td>0.017832</td>\n",
              "      <td>0.253969</td>\n",
              "      <td>0.132908</td>\n",
              "      <td>0.661271</td>\n",
              "      <td>0.040682</td>\n",
              "      <td>1.358219</td>\n",
              "      <td>0.451163</td>\n",
              "      <td>0.155060</td>\n",
              "      <td>0.122951</td>\n",
              "      <td>0.039294</td>\n",
              "      <td>0.056748</td>\n",
              "      <td>0.273152</td>\n",
              "      <td>0.467583</td>\n",
              "      <td>0.185608</td>\n",
              "      <td>...</td>\n",
              "      <td>0.338973</td>\n",
              "      <td>0.133258</td>\n",
              "      <td>0.013520</td>\n",
              "      <td>0.358219</td>\n",
              "      <td>0.338881</td>\n",
              "      <td>0.437434</td>\n",
              "      <td>0.320344</td>\n",
              "      <td>0.115006</td>\n",
              "      <td>0.405984</td>\n",
              "      <td>0.044211</td>\n",
              "      <td>0.116334</td>\n",
              "      <td>0.410443</td>\n",
              "      <td>0.272113</td>\n",
              "      <td>0.112715</td>\n",
              "      <td>0.246117</td>\n",
              "      <td>0.066364</td>\n",
              "      <td>0.680430</td>\n",
              "      <td>0.310476</td>\n",
              "      <td>0.024474</td>\n",
              "      <td>0.017616</td>\n",
              "      <td>0.388681</td>\n",
              "      <td>0.184826</td>\n",
              "      <td>0.298461</td>\n",
              "      <td>0.245282</td>\n",
              "      <td>0.317534</td>\n",
              "      <td>0.749195</td>\n",
              "      <td>0.371180</td>\n",
              "      <td>0.078245</td>\n",
              "      <td>0.148307</td>\n",
              "      <td>0.596257</td>\n",
              "      <td>0.579254</td>\n",
              "      <td>0.319074</td>\n",
              "      <td>0.635639</td>\n",
              "      <td>1.286523</td>\n",
              "      <td>0.025295</td>\n",
              "      <td>0.441679</td>\n",
              "      <td>0.793624</td>\n",
              "      <td>0.178895</td>\n",
              "      <td>1.036580</td>\n",
              "      <td>0.280749</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.786491</td>\n",
              "      <td>0.072849</td>\n",
              "      <td>0.014630</td>\n",
              "      <td>0.032766</td>\n",
              "      <td>0.358258</td>\n",
              "      <td>0.174592</td>\n",
              "      <td>1.094428</td>\n",
              "      <td>1.881274</td>\n",
              "      <td>0.050145</td>\n",
              "      <td>1.622813</td>\n",
              "      <td>0.063235</td>\n",
              "      <td>0.039644</td>\n",
              "      <td>0.068214</td>\n",
              "      <td>0.662752</td>\n",
              "      <td>0.668377</td>\n",
              "      <td>0.468120</td>\n",
              "      <td>1.166782</td>\n",
              "      <td>0.599940</td>\n",
              "      <td>0.038563</td>\n",
              "      <td>0.322086</td>\n",
              "      <td>1.088019</td>\n",
              "      <td>0.065234</td>\n",
              "      <td>0.354067</td>\n",
              "      <td>0.237319</td>\n",
              "      <td>0.104932</td>\n",
              "      <td>0.413898</td>\n",
              "      <td>0.481988</td>\n",
              "      <td>0.264390</td>\n",
              "      <td>0.021328</td>\n",
              "      <td>2.081064</td>\n",
              "      <td>0.103771</td>\n",
              "      <td>0.692999</td>\n",
              "      <td>0.243771</td>\n",
              "      <td>0.040679</td>\n",
              "      <td>0.620579</td>\n",
              "      <td>0.168449</td>\n",
              "      <td>0.011260</td>\n",
              "      <td>0.287366</td>\n",
              "      <td>1.489224</td>\n",
              "      <td>0.320889</td>\n",
              "      <td>...</td>\n",
              "      <td>0.982998</td>\n",
              "      <td>0.572101</td>\n",
              "      <td>0.212113</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.593793</td>\n",
              "      <td>0.417180</td>\n",
              "      <td>0.262409</td>\n",
              "      <td>0.006374</td>\n",
              "      <td>0.106972</td>\n",
              "      <td>0.072427</td>\n",
              "      <td>0.004412</td>\n",
              "      <td>1.437586</td>\n",
              "      <td>1.004017</td>\n",
              "      <td>1.497079</td>\n",
              "      <td>1.797136</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.187073</td>\n",
              "      <td>0.441823</td>\n",
              "      <td>0.176565</td>\n",
              "      <td>2.036337</td>\n",
              "      <td>1.244822</td>\n",
              "      <td>0.792943</td>\n",
              "      <td>0.082841</td>\n",
              "      <td>0.159243</td>\n",
              "      <td>0.878089</td>\n",
              "      <td>0.747523</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.013800</td>\n",
              "      <td>0.005233</td>\n",
              "      <td>1.038241</td>\n",
              "      <td>0.072429</td>\n",
              "      <td>0.590620</td>\n",
              "      <td>1.759950</td>\n",
              "      <td>0.683850</td>\n",
              "      <td>0.012709</td>\n",
              "      <td>0.013401</td>\n",
              "      <td>0.240093</td>\n",
              "      <td>0.047710</td>\n",
              "      <td>0.568559</td>\n",
              "      <td>1.436530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.065886</td>\n",
              "      <td>0.234969</td>\n",
              "      <td>0.076715</td>\n",
              "      <td>0.083571</td>\n",
              "      <td>1.310598</td>\n",
              "      <td>0.075682</td>\n",
              "      <td>0.079029</td>\n",
              "      <td>0.805867</td>\n",
              "      <td>0.027668</td>\n",
              "      <td>0.747283</td>\n",
              "      <td>0.323416</td>\n",
              "      <td>0.260333</td>\n",
              "      <td>0.009031</td>\n",
              "      <td>0.136759</td>\n",
              "      <td>1.008531</td>\n",
              "      <td>0.828211</td>\n",
              "      <td>0.831074</td>\n",
              "      <td>0.129177</td>\n",
              "      <td>0.070943</td>\n",
              "      <td>0.101971</td>\n",
              "      <td>0.570825</td>\n",
              "      <td>0.123184</td>\n",
              "      <td>0.313406</td>\n",
              "      <td>0.121228</td>\n",
              "      <td>0.085739</td>\n",
              "      <td>0.065134</td>\n",
              "      <td>0.180222</td>\n",
              "      <td>0.189006</td>\n",
              "      <td>0.359158</td>\n",
              "      <td>1.306619</td>\n",
              "      <td>0.343667</td>\n",
              "      <td>0.405939</td>\n",
              "      <td>0.017654</td>\n",
              "      <td>0.041998</td>\n",
              "      <td>0.438164</td>\n",
              "      <td>0.094866</td>\n",
              "      <td>0.094330</td>\n",
              "      <td>0.425023</td>\n",
              "      <td>0.511630</td>\n",
              "      <td>0.346295</td>\n",
              "      <td>...</td>\n",
              "      <td>0.491804</td>\n",
              "      <td>0.433973</td>\n",
              "      <td>1.019856</td>\n",
              "      <td>0.155815</td>\n",
              "      <td>0.337042</td>\n",
              "      <td>0.992133</td>\n",
              "      <td>0.853892</td>\n",
              "      <td>0.011171</td>\n",
              "      <td>0.239936</td>\n",
              "      <td>0.168489</td>\n",
              "      <td>0.339766</td>\n",
              "      <td>0.168841</td>\n",
              "      <td>0.796026</td>\n",
              "      <td>1.035537</td>\n",
              "      <td>0.036118</td>\n",
              "      <td>0.036682</td>\n",
              "      <td>0.863123</td>\n",
              "      <td>0.822067</td>\n",
              "      <td>0.547447</td>\n",
              "      <td>0.692856</td>\n",
              "      <td>0.137412</td>\n",
              "      <td>0.499223</td>\n",
              "      <td>0.054049</td>\n",
              "      <td>0.002005</td>\n",
              "      <td>0.669512</td>\n",
              "      <td>0.216869</td>\n",
              "      <td>0.405650</td>\n",
              "      <td>0.083968</td>\n",
              "      <td>0.085510</td>\n",
              "      <td>1.646788</td>\n",
              "      <td>0.007657</td>\n",
              "      <td>1.086805</td>\n",
              "      <td>0.298694</td>\n",
              "      <td>1.197688</td>\n",
              "      <td>0.139057</td>\n",
              "      <td>0.041463</td>\n",
              "      <td>0.287434</td>\n",
              "      <td>0.665317</td>\n",
              "      <td>0.108133</td>\n",
              "      <td>0.022843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.026105</td>\n",
              "      <td>0.388771</td>\n",
              "      <td>0.212297</td>\n",
              "      <td>0.506983</td>\n",
              "      <td>0.402260</td>\n",
              "      <td>0.395627</td>\n",
              "      <td>0.124789</td>\n",
              "      <td>0.109195</td>\n",
              "      <td>0.121822</td>\n",
              "      <td>0.387095</td>\n",
              "      <td>0.230425</td>\n",
              "      <td>0.094370</td>\n",
              "      <td>0.140574</td>\n",
              "      <td>0.536280</td>\n",
              "      <td>0.236456</td>\n",
              "      <td>0.593557</td>\n",
              "      <td>0.329270</td>\n",
              "      <td>0.387014</td>\n",
              "      <td>0.034082</td>\n",
              "      <td>0.302894</td>\n",
              "      <td>0.258651</td>\n",
              "      <td>0.313008</td>\n",
              "      <td>0.581735</td>\n",
              "      <td>0.154137</td>\n",
              "      <td>0.061476</td>\n",
              "      <td>0.603819</td>\n",
              "      <td>0.041385</td>\n",
              "      <td>0.802457</td>\n",
              "      <td>0.484558</td>\n",
              "      <td>0.394392</td>\n",
              "      <td>1.048206</td>\n",
              "      <td>0.293842</td>\n",
              "      <td>0.204873</td>\n",
              "      <td>0.010379</td>\n",
              "      <td>0.427197</td>\n",
              "      <td>0.208337</td>\n",
              "      <td>0.023924</td>\n",
              "      <td>0.196870</td>\n",
              "      <td>0.776506</td>\n",
              "      <td>0.122343</td>\n",
              "      <td>...</td>\n",
              "      <td>0.349069</td>\n",
              "      <td>0.294484</td>\n",
              "      <td>0.686729</td>\n",
              "      <td>0.423441</td>\n",
              "      <td>0.291648</td>\n",
              "      <td>0.711541</td>\n",
              "      <td>0.867561</td>\n",
              "      <td>0.218978</td>\n",
              "      <td>0.476035</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.016342</td>\n",
              "      <td>1.562794</td>\n",
              "      <td>0.224603</td>\n",
              "      <td>0.931255</td>\n",
              "      <td>0.993543</td>\n",
              "      <td>0.116977</td>\n",
              "      <td>0.288325</td>\n",
              "      <td>0.497711</td>\n",
              "      <td>0.066462</td>\n",
              "      <td>0.403095</td>\n",
              "      <td>1.720201</td>\n",
              "      <td>0.611253</td>\n",
              "      <td>0.567270</td>\n",
              "      <td>0.036373</td>\n",
              "      <td>0.631100</td>\n",
              "      <td>0.013022</td>\n",
              "      <td>0.038763</td>\n",
              "      <td>0.150651</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.588477</td>\n",
              "      <td>0.610100</td>\n",
              "      <td>0.037720</td>\n",
              "      <td>0.548847</td>\n",
              "      <td>1.902500</td>\n",
              "      <td>0.766743</td>\n",
              "      <td>0.013931</td>\n",
              "      <td>0.153291</td>\n",
              "      <td>0.340916</td>\n",
              "      <td>0.452838</td>\n",
              "      <td>0.629183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.039578</td>\n",
              "      <td>0.377205</td>\n",
              "      <td>0.305820</td>\n",
              "      <td>0.246477</td>\n",
              "      <td>0.399099</td>\n",
              "      <td>0.347701</td>\n",
              "      <td>0.120734</td>\n",
              "      <td>0.290336</td>\n",
              "      <td>0.142479</td>\n",
              "      <td>0.054465</td>\n",
              "      <td>0.083336</td>\n",
              "      <td>0.339834</td>\n",
              "      <td>0.136768</td>\n",
              "      <td>0.256534</td>\n",
              "      <td>0.024327</td>\n",
              "      <td>0.048493</td>\n",
              "      <td>0.137878</td>\n",
              "      <td>0.055632</td>\n",
              "      <td>0.095250</td>\n",
              "      <td>0.372813</td>\n",
              "      <td>0.089661</td>\n",
              "      <td>0.294316</td>\n",
              "      <td>0.235624</td>\n",
              "      <td>0.396727</td>\n",
              "      <td>0.164776</td>\n",
              "      <td>0.097619</td>\n",
              "      <td>0.070004</td>\n",
              "      <td>0.791301</td>\n",
              "      <td>0.250308</td>\n",
              "      <td>0.521230</td>\n",
              "      <td>0.702172</td>\n",
              "      <td>0.272344</td>\n",
              "      <td>0.271658</td>\n",
              "      <td>0.011512</td>\n",
              "      <td>0.317625</td>\n",
              "      <td>0.451926</td>\n",
              "      <td>0.056299</td>\n",
              "      <td>0.182928</td>\n",
              "      <td>0.246552</td>\n",
              "      <td>0.143378</td>\n",
              "      <td>...</td>\n",
              "      <td>0.048739</td>\n",
              "      <td>0.646854</td>\n",
              "      <td>0.050429</td>\n",
              "      <td>0.206179</td>\n",
              "      <td>0.312552</td>\n",
              "      <td>0.630789</td>\n",
              "      <td>0.273230</td>\n",
              "      <td>0.467586</td>\n",
              "      <td>0.216453</td>\n",
              "      <td>0.005629</td>\n",
              "      <td>0.230794</td>\n",
              "      <td>0.917784</td>\n",
              "      <td>0.029920</td>\n",
              "      <td>0.708837</td>\n",
              "      <td>0.155875</td>\n",
              "      <td>0.573335</td>\n",
              "      <td>0.207120</td>\n",
              "      <td>0.172143</td>\n",
              "      <td>0.283468</td>\n",
              "      <td>0.042468</td>\n",
              "      <td>0.549123</td>\n",
              "      <td>0.282960</td>\n",
              "      <td>0.575955</td>\n",
              "      <td>0.058944</td>\n",
              "      <td>0.555002</td>\n",
              "      <td>0.076770</td>\n",
              "      <td>0.432295</td>\n",
              "      <td>0.454766</td>\n",
              "      <td>0.192723</td>\n",
              "      <td>0.200523</td>\n",
              "      <td>0.723310</td>\n",
              "      <td>0.298673</td>\n",
              "      <td>0.185757</td>\n",
              "      <td>1.632757</td>\n",
              "      <td>0.928632</td>\n",
              "      <td>0.051232</td>\n",
              "      <td>0.079179</td>\n",
              "      <td>0.785074</td>\n",
              "      <td>0.210754</td>\n",
              "      <td>0.538394</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 2048 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0         1         2     ...      2045      2046      2047\n",
              "0  0.186658  0.201167  0.277590  ...  0.090156  0.106479  0.198679\n",
              "1  0.121671  0.334065  0.180439  ...  0.116354  0.276918  0.309094\n",
              "2  0.095625  0.188759  0.102961  ...  0.064262  0.178026  1.056082\n",
              "3  0.192995  0.449393  0.275163  ...  0.086952  0.285739  0.036129\n",
              "4  0.125440  0.335261  0.180668  ...  0.000587  0.710166  0.512944\n",
              "5  0.032018  0.024085  0.264659  ...  0.178895  1.036580  0.280749\n",
              "6  0.786491  0.072849  0.014630  ...  0.047710  0.568559  1.436530\n",
              "7  0.065886  0.234969  0.076715  ...  0.665317  0.108133  0.022843\n",
              "8  0.026105  0.388771  0.212297  ...  0.340916  0.452838  0.629183\n",
              "9  0.039578  0.377205  0.305820  ...  0.785074  0.210754  0.538394\n",
              "\n",
              "[10 rows x 2048 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yNbzL8m6dyA",
        "colab_type": "text"
      },
      "source": [
        "**Lets compare with the Dataframe predictions column**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBzTaWID6aus",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "8d7ad399-fc73-4ec8-a63c-c0b1d36dcda1"
      },
      "source": [
        "resultsDF.head()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>image_name</th>\n",
              "      <th>predictions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>landscapes</td>\n",
              "      <td>night_landscapes_image69.jpg</td>\n",
              "      <td>0.18665837,0.2011668,0.27758992,0.4505339,0.67...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>landscapes</td>\n",
              "      <td>night_landscapes_image82.jpg</td>\n",
              "      <td>0.121671416,0.33406463,0.18043913,0.037964597,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>landscapes</td>\n",
              "      <td>night_landscapes_image1.jpg</td>\n",
              "      <td>0.09562505,0.18875906,0.10296103,0.40914068,0....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>landscapes</td>\n",
              "      <td>night_landscapes_image97.jpg</td>\n",
              "      <td>0.19299471,0.44939288,0.27516326,0.27659455,0....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>landscapes</td>\n",
              "      <td>night_landscapes_image54.jpg</td>\n",
              "      <td>0.12543976,0.3352605,0.18066786,0.22412151,0.3...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        class  ...                                        predictions\n",
              "0  landscapes  ...  0.18665837,0.2011668,0.27758992,0.4505339,0.67...\n",
              "1  landscapes  ...  0.121671416,0.33406463,0.18043913,0.037964597,...\n",
              "2  landscapes  ...  0.09562505,0.18875906,0.10296103,0.40914068,0....\n",
              "3  landscapes  ...  0.19299471,0.44939288,0.27516326,0.27659455,0....\n",
              "4  landscapes  ...  0.12543976,0.3352605,0.18066786,0.22412151,0.3...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FO9zTSft6mx0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a53245be-065c-4378-9b9b-5282dae12592"
      },
      "source": [
        "embdings_np = embdings.to_numpy()\n",
        "y_np = np.array(resultsY)\n",
        "y_np = y_np.reshape(len(y_np),1)\n",
        "# Lets check the first embding to the first row.\n",
        "print(\"Should match with the first record and the first item of predictions column of the table above\" \n",
        "      , str(embdings_np[0][0])\n",
        ")\n",
        "\n",
        "print(y_np[0])\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Should match with the first record and the first item of predictions column of the table above 0.1866583675146103\n",
            "[0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoKeymJW8Yhk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "8d801663-73e8-4f86-dc39-f55f8409f80a"
      },
      "source": [
        "print(embdings_np.shape)\n",
        "print(y_np.shape)\n",
        "\n",
        "data_np = np.concatenate((embdings_np,y_np),axis=1)\n",
        "\n",
        "# Only for visualization\n",
        "data_df = pd.DataFrame(data_np)\n",
        "data_df[data_df[2048].eq(1)].head(1)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(994, 2048)\n",
            "(994, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>2009</th>\n",
              "      <th>2010</th>\n",
              "      <th>2011</th>\n",
              "      <th>2012</th>\n",
              "      <th>2013</th>\n",
              "      <th>2014</th>\n",
              "      <th>2015</th>\n",
              "      <th>2016</th>\n",
              "      <th>2017</th>\n",
              "      <th>2018</th>\n",
              "      <th>2019</th>\n",
              "      <th>2020</th>\n",
              "      <th>2021</th>\n",
              "      <th>2022</th>\n",
              "      <th>2023</th>\n",
              "      <th>2024</th>\n",
              "      <th>2025</th>\n",
              "      <th>2026</th>\n",
              "      <th>2027</th>\n",
              "      <th>2028</th>\n",
              "      <th>2029</th>\n",
              "      <th>2030</th>\n",
              "      <th>2031</th>\n",
              "      <th>2032</th>\n",
              "      <th>2033</th>\n",
              "      <th>2034</th>\n",
              "      <th>2035</th>\n",
              "      <th>2036</th>\n",
              "      <th>2037</th>\n",
              "      <th>2038</th>\n",
              "      <th>2039</th>\n",
              "      <th>2040</th>\n",
              "      <th>2041</th>\n",
              "      <th>2042</th>\n",
              "      <th>2043</th>\n",
              "      <th>2044</th>\n",
              "      <th>2045</th>\n",
              "      <th>2046</th>\n",
              "      <th>2047</th>\n",
              "      <th>2048</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>476</th>\n",
              "      <td>0.247183</td>\n",
              "      <td>0.320458</td>\n",
              "      <td>0.453768</td>\n",
              "      <td>0.029031</td>\n",
              "      <td>0.306225</td>\n",
              "      <td>0.025471</td>\n",
              "      <td>0.004629</td>\n",
              "      <td>0.034889</td>\n",
              "      <td>0.007247</td>\n",
              "      <td>0.407783</td>\n",
              "      <td>0.184695</td>\n",
              "      <td>0.013441</td>\n",
              "      <td>0.115729</td>\n",
              "      <td>0.62249</td>\n",
              "      <td>0.501536</td>\n",
              "      <td>1.561765</td>\n",
              "      <td>0.256776</td>\n",
              "      <td>0.172202</td>\n",
              "      <td>0.119573</td>\n",
              "      <td>0.697882</td>\n",
              "      <td>0.150783</td>\n",
              "      <td>0.100919</td>\n",
              "      <td>0.325939</td>\n",
              "      <td>0.062293</td>\n",
              "      <td>0.31559</td>\n",
              "      <td>0.27122</td>\n",
              "      <td>0.009384</td>\n",
              "      <td>0.099966</td>\n",
              "      <td>0.568195</td>\n",
              "      <td>0.336057</td>\n",
              "      <td>0.62365</td>\n",
              "      <td>0.305853</td>\n",
              "      <td>0.260156</td>\n",
              "      <td>0.036168</td>\n",
              "      <td>0.449206</td>\n",
              "      <td>0.205906</td>\n",
              "      <td>0.03162</td>\n",
              "      <td>0.46075</td>\n",
              "      <td>0.226743</td>\n",
              "      <td>0.499222</td>\n",
              "      <td>...</td>\n",
              "      <td>0.511663</td>\n",
              "      <td>0.126905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.606423</td>\n",
              "      <td>0.018816</td>\n",
              "      <td>0.199596</td>\n",
              "      <td>0.122246</td>\n",
              "      <td>0.084681</td>\n",
              "      <td>0.916436</td>\n",
              "      <td>0.900849</td>\n",
              "      <td>0.013232</td>\n",
              "      <td>0.27873</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.323366</td>\n",
              "      <td>0.909832</td>\n",
              "      <td>0.079706</td>\n",
              "      <td>0.038331</td>\n",
              "      <td>0.134642</td>\n",
              "      <td>0.414657</td>\n",
              "      <td>0.347676</td>\n",
              "      <td>0.016889</td>\n",
              "      <td>0.131062</td>\n",
              "      <td>0.389467</td>\n",
              "      <td>1.151315</td>\n",
              "      <td>0.655024</td>\n",
              "      <td>0.088533</td>\n",
              "      <td>0.151586</td>\n",
              "      <td>1.209917</td>\n",
              "      <td>0.859948</td>\n",
              "      <td>0.149463</td>\n",
              "      <td>0.248916</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.944593</td>\n",
              "      <td>0.164586</td>\n",
              "      <td>0.259422</td>\n",
              "      <td>0.261612</td>\n",
              "      <td>0.308517</td>\n",
              "      <td>0.009843</td>\n",
              "      <td>0.315403</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 2049 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         0         1         2         3     ...      2045      2046      2047  2048\n",
              "476  0.247183  0.320458  0.453768  0.029031  ...  0.308517  0.009843  0.315403   1.0\n",
              "\n",
              "[1 rows x 2049 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SW5_llV_GuQ",
        "colab_type": "text"
      },
      "source": [
        "Once we have the dataset from the  embdings, we will create the distributions to train, test and eval our bottleneck neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jM_lioUHA9Ej",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "193214e4-9b5c-4c14-ddd7-0f1b57b404f8"
      },
      "source": [
        "positives = []\n",
        "negatives = []\n",
        "\n",
        "print(len(data_np))\n",
        "\n",
        "for i in range(0,len(data_np)):\n",
        "  row = data_np[i]\n",
        "  isPositive = int(row[-1]) == 1\n",
        "  if isPositive:\n",
        "    positives.append(row)\n",
        "  else:\n",
        "    negatives.append(row)\n",
        "\n",
        "positives_np = np.array(positives)\n",
        "negatives_np = np.array(negatives)\n",
        "\n",
        "print(\"total positives\",len(positives_np), \"negatives\" ,   len(negatives_np))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "994\n",
            "total positives 434 negatives 560\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3REMXPl-ydU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d8edbf65-9376-498a-e8c9-b6ceab5a2442"
      },
      "source": [
        "\n",
        "positive_distributions = getDistribuitions(positives_np, TRAINING_PERCENTAGE, TESTING_PERCENTAGE, EVAL_PERCENTAGE)\n",
        "negative_distributions = getDistribuitions(negatives_np, TRAINING_PERCENTAGE, TESTING_PERCENTAGE, EVAL_PERCENTAGE)\n",
        "\n",
        "train_positive = positive_distributions[\"train\"]\n",
        "test_positive  = positive_distributions[\"test\"]\n",
        "eval_positive  = positive_distributions[\"eval\"]\n",
        "\n",
        "train_negative = negative_distributions[\"train\"]\n",
        "test_negative  = negative_distributions[\"test\"]\n",
        "eval_negative  = negative_distributions[\"eval\"]\n",
        "\n",
        "print(\"Total of positive distribuitions\",\"train:\",len(train_positive),\"test:\",len(test_positive),\"eval:\",len(eval_positive))\n",
        "print(\"Total of negative distribuitions\",\"train:\",len(train_negative),\"test:\",len(test_negative),\"eval:\",len(eval_negative))\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total of positive distribuitions train: 304 test: 66 eval: 64\n",
            "Total of negative distribuitions train: 392 test: 84 eval: 84\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NkSvlWv-whP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "f5de94cc-6758-4e8d-a203-bb4b58c39be3"
      },
      "source": [
        "train  = np.concatenate((train_positive,train_negative),axis=0)\n",
        "test  = np.concatenate((test_positive,test_negative),axis=0)\n",
        "eval  = np.concatenate((eval_positive,eval_negative),axis=0)\n",
        "\n",
        "print(\"train shape\" , train.shape)\n",
        "print(\"test shape\" , test.shape)\n",
        "print(\"eval shape\" , eval.shape)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train shape (696, 2049)\n",
            "test shape (150, 2049)\n",
            "eval shape (148, 2049)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCUhM1sfF6uq",
        "colab_type": "text"
      },
      "source": [
        "## Train using the embdings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bivwRGcxGA0X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "38dc685a-1f75-45b2-c9e8-02d49edd6be6"
      },
      "source": [
        "def createBottleneckNNModel(inputSize):\n",
        "\n",
        "  model = Sequential()\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Input(shape=(inputSize,)))\n",
        "  model.add(Dense(256, activation='relu'))\n",
        "  model.add(Dropout(0.1))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "inputSize = train.shape[1]-1\n",
        "bottleneckModel = createBottleneckNNModel(inputSize)\n",
        "bottleneckModel.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_2 (Dense)              (None, 256)               524544    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 524,801\n",
            "Trainable params: 524,801\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGtgzGwPHfJP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c08a867e-88e7-43bb-8a93-8a3603435eea"
      },
      "source": [
        "train_x = train[:,0:-1]\n",
        "train_y = train[:,-1]\n",
        "\n",
        "test_x = test[:,0:-1]\n",
        "test_y = test[:,-1]\n",
        "\n",
        "TRAINING_TIME_START = time()\n",
        "progress = bottleneckModel.fit(train_x, train_y,\n",
        "          epochs=EPOCHS_BOTTLENECK,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          validation_data=(test_x, test_y))\n",
        "TRAINING_TIME_END = time()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "47/47 [==============================] - 0s 7ms/step - loss: 0.2372 - accuracy: 0.8994 - val_loss: 0.2494 - val_accuracy: 0.8933\n",
            "Epoch 2/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 0.0896 - accuracy: 0.9655 - val_loss: 0.0954 - val_accuracy: 0.9600\n",
            "Epoch 3/100\n",
            "47/47 [==============================] - 0s 5ms/step - loss: 0.0428 - accuracy: 0.9871 - val_loss: 0.1276 - val_accuracy: 0.9533\n",
            "Epoch 4/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 0.0240 - accuracy: 0.9957 - val_loss: 0.1111 - val_accuracy: 0.9533\n",
            "Epoch 5/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 0.0209 - accuracy: 0.9943 - val_loss: 0.1098 - val_accuracy: 0.9667\n",
            "Epoch 6/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 0.0082 - accuracy: 0.9986 - val_loss: 0.1210 - val_accuracy: 0.9533\n",
            "Epoch 7/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.1087 - val_accuracy: 0.9533\n",
            "Epoch 8/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.1090 - val_accuracy: 0.9667\n",
            "Epoch 9/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.1230 - val_accuracy: 0.9600\n",
            "Epoch 10/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.1153 - val_accuracy: 0.9667\n",
            "Epoch 11/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.1174 - val_accuracy: 0.9667\n",
            "Epoch 12/100\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.1411 - val_accuracy: 0.9533\n",
            "Epoch 13/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.1269 - val_accuracy: 0.9600\n",
            "Epoch 14/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 8.8365e-04 - accuracy: 1.0000 - val_loss: 0.1263 - val_accuracy: 0.9667\n",
            "Epoch 15/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 8.8516e-04 - accuracy: 1.0000 - val_loss: 0.1302 - val_accuracy: 0.9600\n",
            "Epoch 16/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 6.7085e-04 - accuracy: 1.0000 - val_loss: 0.1252 - val_accuracy: 0.9600\n",
            "Epoch 17/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 5.9015e-04 - accuracy: 1.0000 - val_loss: 0.1291 - val_accuracy: 0.9667\n",
            "Epoch 18/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 5.7481e-04 - accuracy: 1.0000 - val_loss: 0.1369 - val_accuracy: 0.9533\n",
            "Epoch 19/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 5.6175e-04 - accuracy: 1.0000 - val_loss: 0.1345 - val_accuracy: 0.9667\n",
            "Epoch 20/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 4.3019e-04 - accuracy: 1.0000 - val_loss: 0.1383 - val_accuracy: 0.9667\n",
            "Epoch 21/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 4.0494e-04 - accuracy: 1.0000 - val_loss: 0.1379 - val_accuracy: 0.9667\n",
            "Epoch 22/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 3.4260e-04 - accuracy: 1.0000 - val_loss: 0.1370 - val_accuracy: 0.9667\n",
            "Epoch 23/100\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 3.5820e-04 - accuracy: 1.0000 - val_loss: 0.1388 - val_accuracy: 0.9667\n",
            "Epoch 24/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 3.8756e-04 - accuracy: 1.0000 - val_loss: 0.1431 - val_accuracy: 0.9667\n",
            "Epoch 25/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 2.8855e-04 - accuracy: 1.0000 - val_loss: 0.1441 - val_accuracy: 0.9667\n",
            "Epoch 26/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 2.4186e-04 - accuracy: 1.0000 - val_loss: 0.1468 - val_accuracy: 0.9667\n",
            "Epoch 27/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 2.4316e-04 - accuracy: 1.0000 - val_loss: 0.1437 - val_accuracy: 0.9600\n",
            "Epoch 28/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 2.4669e-04 - accuracy: 1.0000 - val_loss: 0.1470 - val_accuracy: 0.9667\n",
            "Epoch 29/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 2.0694e-04 - accuracy: 1.0000 - val_loss: 0.1522 - val_accuracy: 0.9533\n",
            "Epoch 30/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 2.2237e-04 - accuracy: 1.0000 - val_loss: 0.1517 - val_accuracy: 0.9667\n",
            "Epoch 31/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.9216e-04 - accuracy: 1.0000 - val_loss: 0.1504 - val_accuracy: 0.9667\n",
            "Epoch 32/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.6023e-04 - accuracy: 1.0000 - val_loss: 0.1565 - val_accuracy: 0.9533\n",
            "Epoch 33/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.9745e-04 - accuracy: 1.0000 - val_loss: 0.1500 - val_accuracy: 0.9667\n",
            "Epoch 34/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.6152e-04 - accuracy: 1.0000 - val_loss: 0.1608 - val_accuracy: 0.9533\n",
            "Epoch 35/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.3918e-04 - accuracy: 1.0000 - val_loss: 0.1557 - val_accuracy: 0.9600\n",
            "Epoch 36/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.2526e-04 - accuracy: 1.0000 - val_loss: 0.1536 - val_accuracy: 0.9667\n",
            "Epoch 37/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.1377e-04 - accuracy: 1.0000 - val_loss: 0.1573 - val_accuracy: 0.9600\n",
            "Epoch 38/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.2112e-04 - accuracy: 1.0000 - val_loss: 0.1609 - val_accuracy: 0.9600\n",
            "Epoch 39/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.0180e-04 - accuracy: 1.0000 - val_loss: 0.1566 - val_accuracy: 0.9667\n",
            "Epoch 40/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.0621e-04 - accuracy: 1.0000 - val_loss: 0.1605 - val_accuracy: 0.9600\n",
            "Epoch 41/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 9.9192e-05 - accuracy: 1.0000 - val_loss: 0.1634 - val_accuracy: 0.9533\n",
            "Epoch 42/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.2924e-04 - accuracy: 1.0000 - val_loss: 0.1598 - val_accuracy: 0.9667\n",
            "Epoch 43/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 9.9406e-05 - accuracy: 1.0000 - val_loss: 0.1648 - val_accuracy: 0.9533\n",
            "Epoch 44/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 8.5819e-05 - accuracy: 1.0000 - val_loss: 0.1593 - val_accuracy: 0.9667\n",
            "Epoch 45/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.0259e-04 - accuracy: 1.0000 - val_loss: 0.1605 - val_accuracy: 0.9667\n",
            "Epoch 46/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 8.1465e-05 - accuracy: 1.0000 - val_loss: 0.1660 - val_accuracy: 0.9600\n",
            "Epoch 47/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 6.9723e-05 - accuracy: 1.0000 - val_loss: 0.1629 - val_accuracy: 0.9667\n",
            "Epoch 48/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 6.6789e-05 - accuracy: 1.0000 - val_loss: 0.1657 - val_accuracy: 0.9600\n",
            "Epoch 49/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 7.4215e-05 - accuracy: 1.0000 - val_loss: 0.1650 - val_accuracy: 0.9667\n",
            "Epoch 50/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 6.2958e-05 - accuracy: 1.0000 - val_loss: 0.1663 - val_accuracy: 0.9667\n",
            "Epoch 51/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 5.8065e-05 - accuracy: 1.0000 - val_loss: 0.1722 - val_accuracy: 0.9600\n",
            "Epoch 52/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 5.5492e-05 - accuracy: 1.0000 - val_loss: 0.1725 - val_accuracy: 0.9600\n",
            "Epoch 53/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 5.3310e-05 - accuracy: 1.0000 - val_loss: 0.1684 - val_accuracy: 0.9667\n",
            "Epoch 54/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 5.3385e-05 - accuracy: 1.0000 - val_loss: 0.1716 - val_accuracy: 0.9600\n",
            "Epoch 55/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 5.0212e-05 - accuracy: 1.0000 - val_loss: 0.1740 - val_accuracy: 0.9600\n",
            "Epoch 56/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 5.3102e-05 - accuracy: 1.0000 - val_loss: 0.1702 - val_accuracy: 0.9667\n",
            "Epoch 57/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 4.5060e-05 - accuracy: 1.0000 - val_loss: 0.1725 - val_accuracy: 0.9667\n",
            "Epoch 58/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 3.7661e-05 - accuracy: 1.0000 - val_loss: 0.1795 - val_accuracy: 0.9600\n",
            "Epoch 59/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 4.0906e-05 - accuracy: 1.0000 - val_loss: 0.1792 - val_accuracy: 0.9600\n",
            "Epoch 60/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 3.7597e-05 - accuracy: 1.0000 - val_loss: 0.1794 - val_accuracy: 0.9600\n",
            "Epoch 61/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 3.3891e-05 - accuracy: 1.0000 - val_loss: 0.1784 - val_accuracy: 0.9667\n",
            "Epoch 62/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 3.2887e-05 - accuracy: 1.0000 - val_loss: 0.1808 - val_accuracy: 0.9600\n",
            "Epoch 63/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 2.6490e-05 - accuracy: 1.0000 - val_loss: 0.1815 - val_accuracy: 0.9600\n",
            "Epoch 64/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 2.7771e-05 - accuracy: 1.0000 - val_loss: 0.1828 - val_accuracy: 0.9600\n",
            "Epoch 65/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 2.8265e-05 - accuracy: 1.0000 - val_loss: 0.1822 - val_accuracy: 0.9600\n",
            "Epoch 66/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 2.9943e-05 - accuracy: 1.0000 - val_loss: 0.1803 - val_accuracy: 0.9667\n",
            "Epoch 67/100\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 2.8992e-05 - accuracy: 1.0000 - val_loss: 0.1837 - val_accuracy: 0.9667\n",
            "Epoch 68/100\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 2.5828e-05 - accuracy: 1.0000 - val_loss: 0.1840 - val_accuracy: 0.9667\n",
            "Epoch 69/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 2.5956e-05 - accuracy: 1.0000 - val_loss: 0.1845 - val_accuracy: 0.9667\n",
            "Epoch 70/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 2.5461e-05 - accuracy: 1.0000 - val_loss: 0.1906 - val_accuracy: 0.9600\n",
            "Epoch 71/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8805e-05 - accuracy: 1.0000 - val_loss: 0.1875 - val_accuracy: 0.9667\n",
            "Epoch 72/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.9889e-05 - accuracy: 1.0000 - val_loss: 0.1924 - val_accuracy: 0.9600\n",
            "Epoch 73/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 2.1960e-05 - accuracy: 1.0000 - val_loss: 0.1944 - val_accuracy: 0.9600\n",
            "Epoch 74/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.9473e-05 - accuracy: 1.0000 - val_loss: 0.1870 - val_accuracy: 0.9667\n",
            "Epoch 75/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 2.1001e-05 - accuracy: 1.0000 - val_loss: 0.1908 - val_accuracy: 0.9600\n",
            "Epoch 76/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.7562e-05 - accuracy: 1.0000 - val_loss: 0.1957 - val_accuracy: 0.9600\n",
            "Epoch 77/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 2.1460e-05 - accuracy: 1.0000 - val_loss: 0.1987 - val_accuracy: 0.9600\n",
            "Epoch 78/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.4689e-05 - accuracy: 1.0000 - val_loss: 0.1907 - val_accuracy: 0.9667\n",
            "Epoch 79/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.3573e-05 - accuracy: 1.0000 - val_loss: 0.1955 - val_accuracy: 0.9600\n",
            "Epoch 80/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.5923e-05 - accuracy: 1.0000 - val_loss: 0.1962 - val_accuracy: 0.9600\n",
            "Epoch 81/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.4755e-05 - accuracy: 1.0000 - val_loss: 0.1965 - val_accuracy: 0.9600\n",
            "Epoch 82/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.3792e-05 - accuracy: 1.0000 - val_loss: 0.1941 - val_accuracy: 0.9667\n",
            "Epoch 83/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.2542e-05 - accuracy: 1.0000 - val_loss: 0.1962 - val_accuracy: 0.9667\n",
            "Epoch 84/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.1741e-05 - accuracy: 1.0000 - val_loss: 0.1975 - val_accuracy: 0.9667\n",
            "Epoch 85/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.6977e-05 - accuracy: 1.0000 - val_loss: 0.1982 - val_accuracy: 0.9667\n",
            "Epoch 86/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.5727e-05 - accuracy: 1.0000 - val_loss: 0.1998 - val_accuracy: 0.9600\n",
            "Epoch 87/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.4943e-05 - accuracy: 1.0000 - val_loss: 0.2022 - val_accuracy: 0.9600\n",
            "Epoch 88/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 8.8804e-06 - accuracy: 1.0000 - val_loss: 0.2043 - val_accuracy: 0.9600\n",
            "Epoch 89/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.9242e-05 - accuracy: 1.0000 - val_loss: 0.1995 - val_accuracy: 0.9600\n",
            "Epoch 90/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.1280e-05 - accuracy: 1.0000 - val_loss: 0.2035 - val_accuracy: 0.9600\n",
            "Epoch 91/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 9.9349e-06 - accuracy: 1.0000 - val_loss: 0.2024 - val_accuracy: 0.9600\n",
            "Epoch 92/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 9.8123e-06 - accuracy: 1.0000 - val_loss: 0.2056 - val_accuracy: 0.9600\n",
            "Epoch 93/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 8.3124e-06 - accuracy: 1.0000 - val_loss: 0.2064 - val_accuracy: 0.9600\n",
            "Epoch 94/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.0515e-05 - accuracy: 1.0000 - val_loss: 0.2029 - val_accuracy: 0.9667\n",
            "Epoch 95/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 9.4079e-06 - accuracy: 1.0000 - val_loss: 0.2039 - val_accuracy: 0.9600\n",
            "Epoch 96/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.8941e-05 - accuracy: 1.0000 - val_loss: 0.2024 - val_accuracy: 0.9600\n",
            "Epoch 97/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 1.1820e-05 - accuracy: 1.0000 - val_loss: 0.2085 - val_accuracy: 0.9600\n",
            "Epoch 98/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 7.3342e-06 - accuracy: 1.0000 - val_loss: 0.2058 - val_accuracy: 0.9667\n",
            "Epoch 99/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 6.1256e-06 - accuracy: 1.0000 - val_loss: 0.2098 - val_accuracy: 0.9600\n",
            "Epoch 100/100\n",
            "47/47 [==============================] - 0s 4ms/step - loss: 9.5520e-06 - accuracy: 1.0000 - val_loss: 0.2064 - val_accuracy: 0.9667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lioX0KwyInkx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "d8618b05-e66f-4625-db66-fd8b7d8b2868"
      },
      "source": [
        "def saveInceptionV3WithBottleneckCheckpointAndDraw():\n",
        "\n",
        "  createFolderIfNotExist(RESULT_FOLDER_WEIGHTS_BOTTLENECK_PATH)\n",
        "  bottleneckModel.save_weights(RESULT_FOLDER_WEIGHTS_BOTTLENECK_FILE_NAME)\n",
        "  \n",
        "  loss              = progress.history[\"loss\"][-1]\n",
        "  val_loss          = progress.history[\"val_loss\"][-1]\n",
        "\n",
        "  accuracy         = progress.history[\"accuracy\"][-1]\n",
        "  val_accuracy     = progress.history[\"val_accuracy\"][-1]\n",
        "\n",
        "  perfData = {\n",
        "        'loss'         :  loss,             \n",
        "        'val_loss'     :  val_loss,    \n",
        "        'accuracy'     :  accuracy,    \n",
        "        'val_accuracy' :  val_accuracy,\n",
        "        'training_time':  (TRAINING_TIME_END - TRAINING_TIME_START),\n",
        "        'train_length' :  len(train_positive) + len(train_negative),\n",
        "        'test_length'  :  len(test_positive ) + len(test_negative )\n",
        "  }\n",
        "\n",
        "  \n",
        "  momoModel.save_weights( DEFAULT_WEIGHTS_FILE_PATH)\n",
        "  plotModelTrainingProgress(progress)\n",
        "  return perfData \n",
        "\n",
        "perfData = saveInceptionV3WithBottleneckCheckpointAndDraw()\n",
        "performanceDF = performanceDF.append(perfData, ignore_index=True)\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9HuqDNVbhHw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "3943910c-6866-40d8-a9d4-c8c938a97685"
      },
      "source": [
        "performanceDF.head()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>test_length</th>\n",
              "      <th>train_length</th>\n",
              "      <th>training_time</th>\n",
              "      <th>val_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.99284</td>\n",
              "      <td>0.025446</td>\n",
              "      <td>94.0</td>\n",
              "      <td>434.0</td>\n",
              "      <td>176.843293</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.016534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>150.0</td>\n",
              "      <td>696.0</td>\n",
              "      <td>19.237642</td>\n",
              "      <td>0.966667</td>\n",
              "      <td>0.206351</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   accuracy      loss  test_length  ...  training_time  val_accuracy  val_loss\n",
              "0   0.99284  0.025446         94.0  ...     176.843293      1.000000  0.016534\n",
              "1   1.00000  0.000010        150.0  ...      19.237642      0.966667  0.206351\n",
              "\n",
              "[2 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTnLPgZVNIYb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "e6683b0b-7db9-4357-9ed9-8e7862ba7e0e"
      },
      "source": [
        "printConfussionMatrix(\n",
        "    ComposedBottleneckModel(\n",
        "          InceptionV3(weights='imagenet', include_top=False, pooling=  'avg'),\n",
        "          bottleneckModel\n",
        "    ) \n",
        ")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing no_momo  total of images:  27\n",
            "100%|██████████| 27/27 [00:05<00:00,  5.25it/s]\n",
            "Processing momo  total of images:  64\n",
            "100%|██████████| 64/64 [00:08<00:00,  7.19it/s]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeCNTm1fgIUR",
        "colab_type": "text"
      },
      "source": [
        "# Final Conclusions and thoughts\n",
        " \n",
        "In this project, I evaluated two approaches to training a model. I got the same confusion matrix for both, however, using the second approach I'd got a significant improvement on training times, train/test acc/loss.\n",
        "\n",
        "I consider the second approach is best because even it was trained with more data in train/set distribution, I got the best time and slightly difference between accuracy/loss on train/set distributions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8oiT_x2lvnK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "95a68d61-a74b-4654-808c-f8ab9dd329da"
      },
      "source": [
        "momoModel.save(RESULT_FOLDER_WEIGHTS_INCEPTION_V3_PATH + \"model\")\n",
        "bottleneckModel.save(RESULT_FOLDER_WEIGHTS_BOTTLENECK_PATH + \"model\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: ./result/inception_v3/model/assets\n",
            "INFO:tensorflow:Assets written to: ./result/inception_v3_bottleneck/model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cZjXTJNl9B9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test if it was saved fine\n",
        "\n",
        "momoModel2       = keras.models.load_model(RESULT_FOLDER_WEIGHTS_INCEPTION_V3_PATH + \"model\")\n",
        "bottleneckModel2 = keras.models.load_model(RESULT_FOLDER_WEIGHTS_BOTTLENECK_PATH + \"model\")"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPs0iJ0oheN7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4088db2a-8e86-4d4c-ac0c-72f1a2473d14"
      },
      "source": [
        "momoModel2.summary()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None, None,  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, None, None, 3 864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, None, None, 3 96          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, None, None, 3 0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, None, None, 3 9216        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, None, None, 3 96          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, None, None, 3 0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, None, None, 6 18432       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, None, None, 6 192         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, None, None, 6 0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, None, None, 6 0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, None, None, 8 5120        max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, None, None, 8 240         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, None, None, 8 0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, None, None, 1 138240      activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, None, None, 1 576         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, None, None, 1 0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, None, None, 1 0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, None, None, 6 12288       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, None, None, 6 192         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, None, None, 6 0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, None, None, 4 9216        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, None, None, 9 55296       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, None, None, 4 144         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, None, None, 9 288         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, None, None, 4 0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, None, None, 9 0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, None, None, 1 0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, None, None, 6 12288       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, None, None, 6 76800       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, None, None, 9 82944       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, None, None, 3 6144        average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, None, None, 6 192         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, None, None, 6 192         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, None, None, 9 288         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, None, None, 3 96          conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, None, None, 6 0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, None, None, 6 0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, None, None, 9 0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, None, None, 3 0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed0 (Concatenate)            (None, None, None, 2 0           activation_5[0][0]               \n",
            "                                                                 activation_7[0][0]               \n",
            "                                                                 activation_10[0][0]              \n",
            "                                                                 activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, None, None, 6 16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, None, None, 6 192         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, None, None, 6 0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, None, None, 4 12288       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, None, None, 9 55296       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, None, None, 4 144         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, None, None, 9 288         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, None, None, 4 0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, None, None, 9 0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, None, None, 2 0           mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, None, None, 6 16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, None, None, 6 76800       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, None, None, 9 82944       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, None, None, 6 16384       average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, None, None, 6 192         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, None, None, 6 192         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, None, None, 9 288         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, None, None, 6 192         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, None, None, 6 0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, None, None, 6 0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, None, None, 9 0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, None, None, 6 0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed1 (Concatenate)            (None, None, None, 2 0           activation_12[0][0]              \n",
            "                                                                 activation_14[0][0]              \n",
            "                                                                 activation_17[0][0]              \n",
            "                                                                 activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, None, None, 6 18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, None, None, 6 192         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, None, None, 6 0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, None, None, 4 13824       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, None, None, 9 55296       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, None, None, 4 144         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, None, None, 9 288         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, None, None, 4 0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, None, None, 9 0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, None, None, 2 0           mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, None, None, 6 18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, None, None, 6 76800       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, None, None, 9 82944       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, None, None, 6 18432       average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, None, None, 6 192         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, None, None, 6 192         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, None, None, 9 288         conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, None, None, 6 192         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, None, None, 6 0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, None, None, 6 0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, None, None, 9 0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, None, None, 6 0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed2 (Concatenate)            (None, None, None, 2 0           activation_19[0][0]              \n",
            "                                                                 activation_21[0][0]              \n",
            "                                                                 activation_24[0][0]              \n",
            "                                                                 activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, None, None, 6 18432       mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, None, None, 6 192         conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, None, None, 6 0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, None, None, 9 55296       activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, None, None, 9 288         conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, None, None, 9 0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, None, None, 3 995328      mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, None, None, 9 82944       activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, None, None, 3 1152        conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, None, None, 9 288         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, None, None, 3 0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, None, None, 9 0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, None, None, 2 0           mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed3 (Concatenate)            (None, None, None, 7 0           activation_26[0][0]              \n",
            "                                                                 activation_29[0][0]              \n",
            "                                                                 max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, None, None, 1 98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, None, None, 1 384         conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, None, None, 1 0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, None, None, 1 114688      activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, None, None, 1 384         conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, None, None, 1 0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, None, None, 1 98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, None, None, 1 114688      activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, None, None, 1 384         conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, None, None, 1 384         conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, None, None, 1 0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, None, None, 1 0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, None, None, 1 114688      activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, None, None, 1 114688      activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, None, None, 1 384         conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, None, None, 1 384         conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, None, None, 1 0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, None, None, 1 0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, None, None, 7 0           mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, None, None, 1 147456      mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, None, None, 1 172032      activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, None, None, 1 172032      activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, None, None, 1 147456      average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, None, None, 1 576         conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, None, None, 1 576         conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, None, None, 1 576         conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, None, None, 1 576         conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, None, None, 1 0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, None, None, 1 0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, None, None, 1 0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, None, None, 1 0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed4 (Concatenate)            (None, None, None, 7 0           activation_30[0][0]              \n",
            "                                                                 activation_33[0][0]              \n",
            "                                                                 activation_38[0][0]              \n",
            "                                                                 activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, None, None, 1 122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, None, None, 1 480         conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, None, None, 1 0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, None, None, 1 179200      activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, None, None, 1 480         conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, None, None, 1 0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, None, None, 1 122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, None, None, 1 179200      activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, None, None, 1 480         conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, None, None, 1 480         conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, None, None, 1 0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, None, None, 1 0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, None, None, 1 179200      activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, None, None, 1 179200      activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, None, None, 1 480         conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, None, None, 1 480         conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, None, None, 1 0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, None, None, 1 0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_4 (AveragePoo (None, None, None, 7 0           mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, None, None, 1 147456      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, None, None, 1 215040      activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, None, None, 1 215040      activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, None, None, 1 147456      average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, None, None, 1 576         conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, None, None, 1 576         conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, None, None, 1 576         conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, None, None, 1 576         conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, None, None, 1 0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, None, None, 1 0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, None, None, 1 0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, None, None, 1 0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed5 (Concatenate)            (None, None, None, 7 0           activation_40[0][0]              \n",
            "                                                                 activation_43[0][0]              \n",
            "                                                                 activation_48[0][0]              \n",
            "                                                                 activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, None, None, 1 122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, None, None, 1 480         conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, None, None, 1 0           batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, None, None, 1 179200      activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, None, None, 1 480         conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, None, None, 1 0           batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, None, None, 1 122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, None, None, 1 179200      activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, None, None, 1 480         conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, None, None, 1 480         conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, None, None, 1 0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, None, None, 1 0           batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, None, None, 1 179200      activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, None, None, 1 179200      activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, None, None, 1 480         conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, None, None, 1 480         conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, None, None, 1 0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, None, None, 1 0           batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_5 (AveragePoo (None, None, None, 7 0           mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, None, None, 1 147456      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, None, None, 1 215040      activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, None, None, 1 215040      activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, None, None, 1 147456      average_pooling2d_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, None, None, 1 576         conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, None, None, 1 576         conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_58 (BatchNo (None, None, None, 1 576         conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_59 (BatchNo (None, None, None, 1 576         conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, None, None, 1 0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, None, None, 1 0           batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, None, None, 1 0           batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, None, None, 1 0           batch_normalization_59[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed6 (Concatenate)            (None, None, None, 7 0           activation_50[0][0]              \n",
            "                                                                 activation_53[0][0]              \n",
            "                                                                 activation_58[0][0]              \n",
            "                                                                 activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, None, None, 1 147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_64 (BatchNo (None, None, None, 1 576         conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, None, None, 1 0           batch_normalization_64[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, None, None, 1 258048      activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_65 (BatchNo (None, None, None, 1 576         conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, None, None, 1 0           batch_normalization_65[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, None, None, 1 147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, None, None, 1 258048      activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_61 (BatchNo (None, None, None, 1 576         conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_66 (BatchNo (None, None, None, 1 576         conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, None, None, 1 0           batch_normalization_61[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, None, None, 1 0           batch_normalization_66[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, None, None, 1 258048      activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, None, None, 1 258048      activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_62 (BatchNo (None, None, None, 1 576         conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_67 (BatchNo (None, None, None, 1 576         conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, None, None, 1 0           batch_normalization_62[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, None, None, 1 0           batch_normalization_67[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_6 (AveragePoo (None, None, None, 7 0           mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, None, None, 1 147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, None, None, 1 258048      activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, None, None, 1 258048      activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, None, None, 1 147456      average_pooling2d_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_60 (BatchNo (None, None, None, 1 576         conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_63 (BatchNo (None, None, None, 1 576         conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_68 (BatchNo (None, None, None, 1 576         conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_69 (BatchNo (None, None, None, 1 576         conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, None, None, 1 0           batch_normalization_60[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, None, None, 1 0           batch_normalization_63[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, None, None, 1 0           batch_normalization_68[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, None, None, 1 0           batch_normalization_69[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed7 (Concatenate)            (None, None, None, 7 0           activation_60[0][0]              \n",
            "                                                                 activation_63[0][0]              \n",
            "                                                                 activation_68[0][0]              \n",
            "                                                                 activation_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, None, None, 1 147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_72 (BatchNo (None, None, None, 1 576         conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_72 (Activation)      (None, None, None, 1 0           batch_normalization_72[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, None, None, 1 258048      activation_72[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_73 (BatchNo (None, None, None, 1 576         conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_73 (Activation)      (None, None, None, 1 0           batch_normalization_73[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, None, None, 1 147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, None, None, 1 258048      activation_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_70 (BatchNo (None, None, None, 1 576         conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_74 (BatchNo (None, None, None, 1 576         conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_70 (Activation)      (None, None, None, 1 0           batch_normalization_70[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_74 (Activation)      (None, None, None, 1 0           batch_normalization_74[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, None, None, 3 552960      activation_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, None, None, 1 331776      activation_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_71 (BatchNo (None, None, None, 3 960         conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_75 (BatchNo (None, None, None, 1 576         conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_71 (Activation)      (None, None, None, 3 0           batch_normalization_71[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_75 (Activation)      (None, None, None, 1 0           batch_normalization_75[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, None, None, 7 0           mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed8 (Concatenate)            (None, None, None, 1 0           activation_71[0][0]              \n",
            "                                                                 activation_75[0][0]              \n",
            "                                                                 max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, None, None, 4 573440      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_80 (BatchNo (None, None, None, 4 1344        conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_80 (Activation)      (None, None, None, 4 0           batch_normalization_80[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, None, None, 3 491520      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, None, None, 3 1548288     activation_80[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_77 (BatchNo (None, None, None, 3 1152        conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_81 (BatchNo (None, None, None, 3 1152        conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_77 (Activation)      (None, None, None, 3 0           batch_normalization_77[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_81 (Activation)      (None, None, None, 3 0           batch_normalization_81[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, None, None, 3 442368      activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, None, None, 3 442368      activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, None, None, 3 442368      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, None, None, 3 442368      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_7 (AveragePoo (None, None, None, 1 0           mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, None, None, 3 409600      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_78 (BatchNo (None, None, None, 3 1152        conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_79 (BatchNo (None, None, None, 3 1152        conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_82 (BatchNo (None, None, None, 3 1152        conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_83 (BatchNo (None, None, None, 3 1152        conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, None, None, 1 245760      average_pooling2d_7[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_76 (BatchNo (None, None, None, 3 960         conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_78 (Activation)      (None, None, None, 3 0           batch_normalization_78[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_79 (Activation)      (None, None, None, 3 0           batch_normalization_79[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_82 (Activation)      (None, None, None, 3 0           batch_normalization_82[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_83 (Activation)      (None, None, None, 3 0           batch_normalization_83[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_84 (BatchNo (None, None, None, 1 576         conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_76 (Activation)      (None, None, None, 3 0           batch_normalization_76[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_0 (Concatenate)          (None, None, None, 7 0           activation_78[0][0]              \n",
            "                                                                 activation_79[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, None, None, 7 0           activation_82[0][0]              \n",
            "                                                                 activation_83[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_84 (Activation)      (None, None, None, 1 0           batch_normalization_84[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed9 (Concatenate)            (None, None, None, 2 0           activation_76[0][0]              \n",
            "                                                                 mixed9_0[0][0]                   \n",
            "                                                                 concatenate[0][0]                \n",
            "                                                                 activation_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, None, None, 4 917504      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_89 (BatchNo (None, None, None, 4 1344        conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_89 (Activation)      (None, None, None, 4 0           batch_normalization_89[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, None, None, 3 786432      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, None, None, 3 1548288     activation_89[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_86 (BatchNo (None, None, None, 3 1152        conv2d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_90 (BatchNo (None, None, None, 3 1152        conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_86 (Activation)      (None, None, None, 3 0           batch_normalization_86[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_90 (Activation)      (None, None, None, 3 0           batch_normalization_90[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, None, None, 3 442368      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, None, None, 3 442368      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, None, None, 3 442368      activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, None, None, 3 442368      activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_8 (AveragePoo (None, None, None, 2 0           mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, None, None, 3 655360      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_87 (BatchNo (None, None, None, 3 1152        conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_88 (BatchNo (None, None, None, 3 1152        conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_91 (BatchNo (None, None, None, 3 1152        conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_92 (BatchNo (None, None, None, 3 1152        conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, None, None, 1 393216      average_pooling2d_8[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_85 (BatchNo (None, None, None, 3 960         conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_87 (Activation)      (None, None, None, 3 0           batch_normalization_87[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_88 (Activation)      (None, None, None, 3 0           batch_normalization_88[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_91 (Activation)      (None, None, None, 3 0           batch_normalization_91[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_92 (Activation)      (None, None, None, 3 0           batch_normalization_92[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_93 (BatchNo (None, None, None, 1 576         conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_85 (Activation)      (None, None, None, 3 0           batch_normalization_85[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_1 (Concatenate)          (None, None, None, 7 0           activation_87[0][0]              \n",
            "                                                                 activation_88[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, None, None, 7 0           activation_91[0][0]              \n",
            "                                                                 activation_92[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_93 (Activation)      (None, None, None, 1 0           batch_normalization_93[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed10 (Concatenate)           (None, None, None, 2 0           activation_85[0][0]              \n",
            "                                                                 mixed9_1[0][0]                   \n",
            "                                                                 concatenate_1[0][0]              \n",
            "                                                                 activation_93[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d (Globa (None, 2048)         0           mixed10[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1024)         2098176     global_average_pooling2d[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 1024)         0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1)            1025        dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 23,901,985\n",
            "Trainable params: 23,867,553\n",
            "Non-trainable params: 34,432\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZL7vcvthhfH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "07cf28ee-3019-45e0-867c-b2780cb63813"
      },
      "source": [
        "bottleneckModel2.summary()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_2 (Dense)              (None, 256)               524544    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 524,801\n",
            "Trainable params: 524,801\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-zpWxUZgqf0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "135c682c-bdd3-4547-a573-55300dbcec03"
      },
      "source": [
        "\n",
        "!rm -rf dist dist.zip\n",
        "!mkdir -p dist/inception_v3\n",
        "!mkdir -p dist/bottleneck\n",
        "\n",
        "!cp -r result/inception_v3/model dist/inception_v3\n",
        "!cp -r result/inception_v3_bottleneck/model dist/bottleneck\n",
        "\n",
        "!zip -r dist.zip dist"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: dist/ (stored 0%)\n",
            "  adding: dist/bottleneck/ (stored 0%)\n",
            "  adding: dist/bottleneck/model/ (stored 0%)\n",
            "  adding: dist/bottleneck/model/variables/ (stored 0%)\n",
            "  adding: dist/bottleneck/model/variables/variables.index (deflated 60%)\n",
            "  adding: dist/bottleneck/model/variables/variables.data-00000-of-00001 (deflated 14%)\n",
            "  adding: dist/bottleneck/model/assets/ (stored 0%)\n",
            "  adding: dist/bottleneck/model/saved_model.pb (deflated 87%)\n",
            "  adding: dist/inception_v3/ (stored 0%)\n",
            "  adding: dist/inception_v3/model/ (stored 0%)\n",
            "  adding: dist/inception_v3/model/variables/ (stored 0%)\n",
            "  adding: dist/inception_v3/model/variables/variables.index (deflated 79%)\n",
            "  adding: dist/inception_v3/model/variables/variables.data-00000-of-00001 (deflated 7%)\n",
            "  adding: dist/inception_v3/model/assets/ (stored 0%)\n",
            "  adding: dist/inception_v3/model/saved_model.pb (deflated 93%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqmEO7_jkTLe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "632bd4f4-2d58-4b36-ad7e-8cb35005c0f9"
      },
      "source": [
        "if GOOGLE_COLLAB:\n",
        "  from google.colab import files\n",
        "  files.download(\"dist.zip\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_c0db21fb-0820-432a-a6ee-d61baeb4bbc3\", \"dist.zip\", 110447508)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXqWx3bYiSUm",
        "colab_type": "text"
      },
      "source": [
        "# Next step\n",
        "\n",
        "1. If you  ran this notebook on Colab, DO NOT FORGET TO DOWNLOAD the dist folder and start to use your model clasificator for images that contains momo\n",
        "\n",
        "2. There is other notebook called detect_momo_in_videos that shows how to use our bottleneck model to detect momo images in videos."
      ]
    }
  ]
}